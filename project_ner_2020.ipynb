{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Named entity recognition project</h1>\n",
    "<h3>(2020 spring semester)</h3><br>\n",
    "The content of this project will be covered in the upcoming lectures. Specifically, the NER task will be discussed today (26.3), and the deep contextual representations will be covered towards the end of the course.<br><br>\n",
    "In this project the goal is to detect persons, organizations and locations mentioned in the text. Thus, we are doing predictions for each word/token. The data looks something like this:\n",
    "<table>\n",
    "\t<tr>\n",
    "\t\t<th>Word</th>\n",
    "\t\t<th>Label</th>\n",
    " \t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Onneksi</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    " \t<tr>\n",
    "\t\t<td>sitä</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>ei</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>missään</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>vaiheessa</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    "    <tr>\n",
    "\t\t<td>restauroitu</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>samalla</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>tavoin</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    "    <tr>\n",
    "\t\t<td>kuin</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    "    <tr>\n",
    "\t\t<td>Uppsalan</td>\n",
    "\t\t<td>B-FAC</td>\n",
    " \t</tr>\n",
    "    <tr>\n",
    "\t\t<td>tuomiokirkkoa</td>\n",
    "\t\t<td>I-FAC</td>\n",
    " \t</tr>\n",
    "    <tr>\n",
    "\t\t<td>.</td>\n",
    "\t\t<td>O</td>\n",
    " \t</tr>\n",
    "</table>\n",
    "(Translation: Fortunately, it was never restored in the same way as the Uppsala Cathedral.)<br><br>\n",
    "Here the label `B-FAC` and `I-FAC` refer to the beginning and the rest of a building-named entity, respectively. Words that are not a part of a named entity are labeled with the `O` tag. A list of the labels used can be found here: https://spacy.io/api/annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data</h3><br>\n",
    "We use the in-development data of the Turku NER Corpus. The data can be found here: https://github.com/TurkuNLP/turku-ner-corpus/tree/development/combined-extended\n",
    "<!--These data combine the FiNER and Turku NER corpus datasets. The annotations are extended toward compatibility with OntoNotes corpus NE annotation.-->\n",
    "<br><br>\n",
    "If you prefer to have the whole git repository on your local machine, you can first git clone the repository `git clone https://github.com/TurkuNLP/turku-ner-corpus.git`, change the working directory, and switch to the development branch using the comment `git checkout development`. The data are found in the `combined-extended` directory.\n",
    "<br><br>\n",
    "There is also English data in similar formats available. Due to restrictions on redistribution, please contact us if you would like to use the English data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Milestones</h3>\n",
    "<h4>1.1 Predicting word labels independently</h4><br>\n",
    "The first part is to train a classifier which assigns a label for each given input word independently. Evaluate the results on token level and entity level. Report your results with different network hyperparameters. Also discuss whether the token level accuracy is a reasonable metric.\n",
    "<h4>1.2 Expand context</h4><br>\n",
    "Modify your network in such way that it is able to utilize the surrounding context of the word. This can be done for instance with a convolutional or recurrent layer. Analyze different neural network architectures and hyperparameters. How does utilizing the surrounding context influence the predictions?\n",
    "<h4>2.1 Use deep contextual representations</h4><br>\n",
    "Use deep contextual representations. Fine-tune the embeddings with different hyperparameters. Try different models (e.g. cased and uncased, multilingual BERT). Report your results.\n",
    "<h4>2.2 Error analysis</h4><br>\n",
    "Select one model from each of the previous milestones (three models in total). Look at the entities these models predict. Analyze the errors made. Are there any patterns? How do the errors one model makes differ from those made by another?\n",
    "<h4>3.1 Predictions on unannotated text</h4><br>\n",
    "Use the three models selected in milestone 2.2 to do predictions on the [sampled wikipedia text](http://dl.turkunlp.org/TKO_8965-projects/ner/).\n",
    "<h4>3.2 Statistically analyze the results</h4><br>\n",
    "Statistically analyze and compare the predictions. You can, for example, analyze if some models tend to predict more entities starting with a capital letter, or if some models predict more entities for some specific classes than others.\n",
    "<!--Look at the entities your models predict. Can you identify any systematic errors? Are the systematic errors the same as the ones found in milestone 2.2?-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

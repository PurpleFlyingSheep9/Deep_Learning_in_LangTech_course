{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "seq2label_conv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R_po4ZHGFTw",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Label with conv-nets\n",
        "\n",
        "## Goals of the lecture\n",
        "\n",
        "1. Understand seq2label problem setting\n",
        "2. Understanding how 1d-convolution and its practicalities work\n",
        "3. Acquiring capability to implement seq2label models with conv-nets on keras\n",
        "\n",
        "## Outline\n",
        "\n",
        "0. Intro\n",
        "1. Few words about seq2label, data and its form\n",
        "2. Detailed look into a simple example model\n",
        "3. Look into a more complicated model\n",
        "4. Look into what might be happening here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtntD_7KGFTw",
        "colab_type": "text"
      },
      "source": [
        "## Seq2label\n",
        "\n",
        "\n",
        "### Recap on Bag-Of-Words\n",
        "\n",
        "\n",
        "#### Input\n",
        "\n",
        "As was previously discussed, when we are bulding a bag of words - classifier, our input is a set of unordered words and the output is a single label. \n",
        "\n",
        "The list of words is given to the model as a feature vector, a single numerical vector. So, to repeat its input is a single vector.\n",
        "\n",
        "To do this transformation, all tokens need to have a numerical identification. That is, each word is associated with a specific number. In practice this is retrieved from a vocabulary as was shown before.\n",
        "\n",
        "### Output\n",
        "\n",
        "Its output is a label. As such it is a function from a vector into a label.\n",
        "\n",
        "Labels are typically represented as a vector, which has a value for all possible labels, or a single value if the task is binary.\n",
        "\n",
        "### How Seq2Label differs from this?\n",
        "\n",
        "The only difference from the bow is the form of the input.\n",
        "\n",
        "Like in the bow example, the input is a single vector, but instead of representing a set, it represents a sequence. This is achieved simply by adding the token identifiers after each other.\n",
        "\n",
        "The output is identical.\n",
        "\n",
        "### A Simple Example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19PR76eLGFTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%script bash\n",
        "\n",
        "#uncomment this stuff to download data\n",
        "#mkdir -p data\n",
        "#cd data\n",
        "#wget --quiet https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "#unzip wiki-news-300d-1M.vec.zip\n",
        "#wget https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/raw/master/data/imdb_train.json\n",
        "#cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRqRcOtqGFTw",
        "colab_type": "code",
        "outputId": "b3cefd68-36d6-4192-e486-0e99af6b2baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#Let's define a simple and short vocabulary\n",
        "tokens = list(set('it is essential the cat sat on the mat'.split()))\n",
        "token_ids = {}\n",
        "for i,t in enumerate(tokens):\n",
        "    print (i,':',t)\n",
        "    token_ids[t] = i"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 : it\n",
            "1 : mat\n",
            "2 : on\n",
            "3 : sat\n",
            "4 : the\n",
            "5 : essential\n",
            "6 : cat\n",
            "7 : is\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lXGjOq9GFVU",
        "colab_type": "code",
        "outputId": "c8bddb34-5e7f-41cf-e781-171149dede6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Let's define a sentence we want to turn into a vector form\n",
        "\n",
        "sentence = 'cat sat on the mat'.split()\n",
        "print(sentence)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cat', 'sat', 'on', 'the', 'mat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC2lMtPaGFVU",
        "colab_type": "code",
        "outputId": "b5da779f-4d6d-4d5c-d01b-ecebd0e32bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Bag of words vector\n",
        "import numpy as np\n",
        "\n",
        "#Vector is as long as the vocabulary, initial values are zeros\n",
        "vector = np.zeros((len(token_ids),))\n",
        "for token in sentence:\n",
        "    vector[token_ids[token]] = 1.0\n",
        "print (vector)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 1. 1. 1. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bRWVVyHGFVU",
        "colab_type": "code",
        "outputId": "730815c6-74d4-4e1b-c3dc-5cb1e856ecad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Sequential input vector\n",
        "\n",
        "vector = [token_ids[t] for t in sentence]\n",
        "print (vector)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6, 3, 2, 4, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Hn2vneGFVU",
        "colab_type": "text"
      },
      "source": [
        "### Padding of the data\n",
        "\n",
        "Some deep-learning frameworks, such as keras, require the input to have a preset shape which is the same for all examples. This is not the case with all frameworks, but its good to cover. If for nothing else, for the sake of this tutorial.\n",
        "\n",
        "Since these input sequences are not the same length, we need to make them so. This is accomplished by padding, which in practice means adding zeros to the input vectors to make them uniform in size\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFTh7Y9zGFVU",
        "colab_type": "code",
        "outputId": "de283ddb-8979-4f9d-efc6-19b3f349070a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Padding can easily be achieved with keras in-built functions\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "pad_sequences(np.array([vector]), maxlen=20, padding='post')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6, 3, 2, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpWDMRf2GFVU",
        "colab_type": "text"
      },
      "source": [
        "### Super short recap on embedding layers\n",
        "\n",
        "Most often these sequences of numbers are used to load embeddings, which are used as the input of the next stage of the network. An embedding layer could be imagined as a dictionary; in go indexes, out comes representative embeddings.\n",
        "\n",
        "What happens inside an embedding layer can be demonstrated with numpy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Cd1ldAGFVU",
        "colab_type": "code",
        "outputId": "75f1b1ee-ab74-4981-c8c0-2ad167dfbfc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import numpy as np\n",
        "embedding_size = 4\n",
        "#Let's just make a random matrix to act as our demonstration embedding matrix\n",
        "embedding_matrix = np.random.rand(len(token_ids),embedding_size)\n",
        "\n",
        "print ('from:', vector)\n",
        "print ('into:')\n",
        "print (embedding_matrix[vector])\n",
        "print ('and the matrix has a shape:', embedding_matrix[vector].shape)\n",
        "\n"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from: [6, 3, 2, 4, 1]\n",
            "into:\n",
            "[[0.10726681 0.74655373 0.6596653  0.39774951]\n",
            " [0.85181484 0.73395423 0.95378871 0.23548018]\n",
            " [0.5142698  0.44822745 0.08438433 0.68896755]\n",
            " [0.66398121 0.15436887 0.59714718 0.15909622]\n",
            " [0.51892173 0.37764716 0.65313466 0.73405023]]\n",
            "and the matrix has a shape: (5, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFNY2O8oGFVU",
        "colab_type": "text"
      },
      "source": [
        "## Convolution recap\n",
        "\n",
        "### What's good about convolution?\n",
        "\n",
        "Convolutional networks are fast. They are fast because they are easily parallelizable. That is because no part of the convolution output depends on another one of the outputs. If that was the case, as it is in recurrent neural networks we couldn't make predictions before other parts of the layer have finished calculating.\n",
        "\n",
        "### 1-dimensional convolution\n",
        "\n",
        "During he earlier lecture and also to a large degree on the internet, image classification is used to demonstrate conv-nets. That is very understandable, for that is the area in which these networks traditionally shine and also because the convolution operation is a traditional tool for image processing.\n",
        "\n",
        "An image is a two dimensional input. It has height and it has width. A sequence of words has only one dimension. That is, you can traverse it forwards and backwards, but there is no traversing it \"up\" or \"down\" nor is there \"toward\" and \"from\" etc. It just is a one-dimensional sequence of tokens.\n",
        "\n",
        "Because of this, the convolution operation applied to text is usally one dimensional, instead of two dimensional convolution often applied to images.\n",
        "\n",
        "The relevant variables in the operation are:\n",
        "\n",
        "1. Filter size, in effect how many conv operations are applied to the input\n",
        "2. Kernel size, in effect how wide the applied conv operations are\n",
        "3. Stride length\n",
        "4. Padding\n",
        "\n",
        "#### Input / Output\n",
        "\n",
        "This operation takes as its input 2-d matrices (when we discount the batch_size). The matrix is in the shape of (steps, input_dim)\n",
        "\n",
        "The operation returns similarly a 2-d matrix. The size of the output matrix is (new_steps, filters). In this shape filters correspond to the amount of filters used. The new step count is a little bit trickier. It depends on stride length, kernel size and used padding style. \n",
        "https://keras.io/layers/convolutional/#conv1d\n",
        "\n",
        "\n",
        "### Illustration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ9wiia1GFVU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "![title](Untitled.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2oLwA16GFVU",
        "colab_type": "text"
      },
      "source": [
        "## 1-dimensional convolution and text\n",
        "\n",
        "Since the convolutional network bases its activations on a window of certain length, it is reacting to snippets of text the size of the convolution window. For example a conv-net with a window size of three, is capable of reacting to trigrams etc. In this sense convolutional text classifiers remind us of bag-of-ngram classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mzuf9auOP5E",
        "colab_type": "code",
        "outputId": "c532a79d-9b10-43cb-cd5e-a0152dcc0396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import json\n",
        "inf = open('data/imdb_train.json','rt')\n",
        "ebin = json.load(inf)\n",
        "inf.close()\n",
        "\n",
        "\n",
        "print (ebin[0])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'class': 'pos', 'text': \"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.  Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.  The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.  Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.  Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSJhuzWqGFW4",
        "colab_type": "text"
      },
      "source": [
        "### Pooling\n",
        "\n",
        "The pooling operation is used for dimensionality reduction of the input. The type of pooling discussed in this lecture is max-pooling, but other variants exist. In the context of this lecture we will be using global pooling, similarly variants exist.\n",
        "\n",
        "Global, 1 dimensional, max pooling operation takes as its input a 2-d matrix and returns a vector. \n",
        "\n",
        "This operation is best explained with an illustration.\n",
        "\n",
        "### Illustration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vs_IdIjGFW4",
        "colab_type": "code",
        "outputId": "06bdb244-8e86-4c86-8e51-041a96f2a1be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#We can illustrate global max pooling with numpy\n",
        "embeddings = embedding_matrix[vector]\n",
        "print ('we start with:')\n",
        "print (embeddings)\n",
        "print ()\n",
        "print ('after max pooling:')\n",
        "print (np.max(embeddings, axis=0))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we start with:\n",
            "[[0.10726681 0.74655373 0.6596653  0.39774951]\n",
            " [0.85181484 0.73395423 0.95378871 0.23548018]\n",
            " [0.5142698  0.44822745 0.08438433 0.68896755]\n",
            " [0.66398121 0.15436887 0.59714718 0.15909622]\n",
            " [0.51892173 0.37764716 0.65313466 0.73405023]]\n",
            "\n",
            "after max pooling:\n",
            "[0.85181484 0.74655373 0.95378871 0.73405023]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYWDolCcGFW4",
        "colab_type": "text"
      },
      "source": [
        "### ReLU\n",
        "\n",
        "Rectified linear unit. An activation used often with conv-nets. As an activation it doesn't care about input size. Input and output are always of similar shape. Simply returns the value if it is >0 and 0 otherwise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQPztM2AGFW4",
        "colab_type": "text"
      },
      "source": [
        "## The simple model\n",
        "\n",
        "To start of this lecture, let us look into an example convnet-text classifier included in keras examples: https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py\n",
        "\n",
        "It uses the IMDB-dataset already we are already familiar with and contains a single convolution layer. It's performance is said to be 0.89, which is somewhat disappointingly less than the bag-of-words example earlier.\n",
        "\n",
        "In addition to demonstrating a convolutional text classifier, the example nicely illuminates what typical deep-learning code looks like.\n",
        "\n",
        "For purposes of simplification I've yet very slightly simplified the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6hANxnFOmOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "d5b0a745-e499-4df0-b2e9-37677c0c7be9"
      },
      "source": [
        "import json\n",
        "import random\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "with open(\"data/imdb_train.json\") as f:\n",
        "    data=json.load(f)\n",
        "# We need to gather the texts, into a list\n",
        "texts=[one_example[\"text\"] for one_example in data]\n",
        "labels=[one_example[\"class\"] for one_example in data]\n",
        "label_encoder=LabelEncoder() #Turns class labels into integers\n",
        "class_numbers=label_encoder.fit_transform(labels)\n",
        "\n",
        "max_features = 5000\n",
        "#Let's make the sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(texts) \n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "#Let's pad this\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "max_len = 400\n",
        "padded_sequences = pad_sequences(sequences, maxlen=400, padding='post')\n",
        "\n",
        "#And let's have a look!\n",
        "print ('sequence')\n",
        "print (sequences[0])\n",
        "print ('padded sequence')\n",
        "print(padded_sequences[0])\n",
        "\n",
        "#And then cut out a development set out of the data for us to see how well we do\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences,\n",
        "                                                    class_numbers, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequence\n",
            "[15, 28, 10, 534, 166, 176, 29, 1, 557, 15, 203, 641, 2614, 5, 23, 224, 145, 1, 1027, 658, 129, 2, 46, 292, 1, 2, 292, 170, 275, 9, 39, 177, 5, 75, 3, 809, 2615, 79, 10, 228, 33, 9, 193, 12, 62, 642, 7, 1, 4251, 39, 5, 275, 93, 52, 57, 326, 722, 25, 6, 2511, 38, 1350, 6, 169, 169, 787, 18, 59, 9, 373, 166, 5, 63, 29, 1, 433, 50, 8, 12, 1815, 621, 45, 4, 8, 43, 1298, 3431, 40, 543, 945, 1, 3512, 2, 78, 1, 573, 745, 4, 1663, 22, 74, 2005, 1155, 17, 4, 260, 10, 6, 28, 40, 484, 1877, 34, 890, 21, 2587, 36, 7, 549, 91, 21, 22, 166, 5, 779, 10, 2, 165, 8, 353, 45, 199, 679, 31, 14, 5, 1, 227, 4, 10, 16, 17, 2, 87, 4, 23, 447, 58, 131, 11, 25, 89, 8, 14, 1, 447, 59, 44, 279, 6, 62, 323, 4, 86, 1, 775, 787, 18, 223, 50, 8, 413, 513, 6, 60, 19, 14, 887, 230, 38, 34, 1, 3536, 1669, 716, 2, 910, 6, 1074, 13, 3, 28, 971, 1388, 1630, 134, 25, 489, 347, 34, 74, 6, 720, 68, 84, 23, 2453, 910, 105, 11, 25, 469, 80, 5, 120, 8, 6, 25, 33, 6, 1663, 519, 34, 9, 275, 25, 39, 4137, 224, 771, 4, 642, 179, 7, 10, 36, 1582, 79, 3, 515, 2, 3, 2352, 2, 1, 222, 2118, 2717, 716, 78, 1, 163, 211, 24, 65, 1, 4, 3, 50, 8, 381, 5, 1417, 1, 74, 716, 13, 627, 903, 779, 776, 15, 27, 550, 383, 580, 3, 222, 757, 4, 94, 3432, 3, 1310, 832, 132, 1320, 343, 10, 16, 6, 14, 80, 33, 36, 19, 27, 645, 38, 156, 59, 9, 100, 6, 87, 80, 44, 20, 91, 784, 241, 8, 123, 349, 2, 198, 121, 3, 745, 2, 3605, 2063, 7, 10, 16, 6, 3, 246, 484, 1877, 6, 367, 27, 4, 1, 87, 1015, 80, 122, 5, 1692, 10, 1219, 17, 6, 25, 2511, 69, 15, 28, 1, 687, 203, 516, 10, 871, 69, 9, 88, 120, 84, 80, 66, 26, 271, 492, 4557, 3583, 9, 120, 10, 14, 3, 188, 25, 6, 341, 31, 572, 323, 17, 374, 228, 38, 27, 4, 1, 87, 9, 436, 25, 6, 20, 1, 1558]\n",
            "padded sequence\n",
            "[  15   28   10  534  166  176   29    1  557   15  203  641 2614    5\n",
            "   23  224  145    1 1027  658  129    2   46  292    1    2  292  170\n",
            "  275    9   39  177    5   75    3  809 2615   79   10  228   33    9\n",
            "  193   12   62  642    7    1 4251   39    5  275   93   52   57  326\n",
            "  722   25    6 2511   38 1350    6  169  169  787   18   59    9  373\n",
            "  166    5   63   29    1  433   50    8   12 1815  621   45    4    8\n",
            "   43 1298 3431   40  543  945    1 3512    2   78    1  573  745    4\n",
            " 1663   22   74 2005 1155   17    4  260   10    6   28   40  484 1877\n",
            "   34  890   21 2587   36    7  549   91   21   22  166    5  779   10\n",
            "    2  165    8  353   45  199  679   31   14    5    1  227    4   10\n",
            "   16   17    2   87    4   23  447   58  131   11   25   89    8   14\n",
            "    1  447   59   44  279    6   62  323    4   86    1  775  787   18\n",
            "  223   50    8  413  513    6   60   19   14  887  230   38   34    1\n",
            " 3536 1669  716    2  910    6 1074   13    3   28  971 1388 1630  134\n",
            "   25  489  347   34   74    6  720   68   84   23 2453  910  105   11\n",
            "   25  469   80    5  120    8    6   25   33    6 1663  519   34    9\n",
            "  275   25   39 4137  224  771    4  642  179    7   10   36 1582   79\n",
            "    3  515    2    3 2352    2    1  222 2118 2717  716   78    1  163\n",
            "  211   24   65    1    4    3   50    8  381    5 1417    1   74  716\n",
            "   13  627  903  779  776   15   27  550  383  580    3  222  757    4\n",
            "   94 3432    3 1310  832  132 1320  343   10   16    6   14   80   33\n",
            "   36   19   27  645   38  156   59    9  100    6   87   80   44   20\n",
            "   91  784  241    8  123  349    2  198  121    3  745    2 3605 2063\n",
            "    7   10   16    6    3  246  484 1877    6  367   27    4    1   87\n",
            " 1015   80  122    5 1692   10 1219   17    6   25 2511   69   15   28\n",
            "    1  687  203  516   10  871   69    9   88  120   84   80   66   26\n",
            "  271  492 4557 3583    9  120   10   14    3  188   25    6  341   31\n",
            "  572  323   17  374  228   38   27    4    1   87    9  436   25    6\n",
            "   20    1 1558    0    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLWjuPOuGFW4",
        "colab_type": "text"
      },
      "source": [
        "### Simple model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi7buTArGFW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "ec3aca50-7452-4238-80b4-358b0ccdca32"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, Input\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# set parameters:\n",
        "max_features = 5000\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 100\n",
        "filters = 150\n",
        "kernel_size = 3\n",
        "epochs = 5\n",
        "\n",
        "print('Loading data...')\n",
        "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print (x_train.shape)\n",
        "\n",
        "#Since we are using output the size of 2, we will have to do one-hot encoding\n",
        "#x_test = to_categorical(x_test)\n",
        "#y_test = to_categorical(y_test)\n",
        "\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "#xx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "#xx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "oh_y_test = onehot_encoder.fit_transform(y_test.reshape(-1, 1))\n",
        "oh_y_train = onehot_encoder.transform(y_train.reshape(-1, 1))\n",
        "\n",
        "\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "#Let's define the inputs\n",
        "x = Input(shape=(maxlen,))\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "\n",
        "embedding_layer = Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=maxlen)\n",
        "\n",
        "embeddings = embedding_layer(x)\n",
        "\n",
        "conv_layer = Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)\n",
        "conv_result = conv_layer(embeddings)\n",
        "pooled = (GlobalMaxPooling1D())(conv_result) \n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "out = Dense(2, activation='softmax')(pooled)\n",
        "\n",
        "model = Model(x, out)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, oh_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, oh_y_test))\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "20000 train sequences\n",
            "5000 test sequences\n",
            "(20000, 400)\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (20000, 400)\n",
            "x_test shape: (5000, 400)\n",
            "Build model...\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 70s 4ms/step - loss: 0.4068 - acc: 0.8105 - val_loss: 0.2880 - val_acc: 0.8728\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 0.2116 - acc: 0.9163 - val_loss: 0.2699 - val_acc: 0.8876\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 0.1135 - acc: 0.9648 - val_loss: 0.2764 - val_acc: 0.8870\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 68s 3ms/step - loss: 0.0475 - acc: 0.9899 - val_loss: 0.3103 - val_acc: 0.8916\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 69s 3ms/step - loss: 0.0159 - acc: 0.9988 - val_loss: 0.3504 - val_acc: 0.8926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa16d84feb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKeEtDo7GFW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(model,show_shapes=True,show_layer_names=False).create(prog='dot', format='svg'))\n",
        "#model_to_dot(model,show_shapes=True,show_layer_names=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5znCNsSGFW4",
        "colab_type": "text"
      },
      "source": [
        "## What if we were to load pre-trained embeddings?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llc3fpELGFW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmkU4mW2GFW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "7b532dd3-fe2f-4d99-b5fd-f720cc9d7f9c"
      },
      "source": [
        "#vocab\n",
        "\n",
        "{'chitre': 70850,\n",
        " 'moviewise': 58717,\n",
        " 'looonnnggg': 80846,\n",
        " 'salomé': 80275,\n",
        " 'caricaturish': 53075,\n",
        " \"'shower'\": 66431,}"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'shower'\": 66431,\n",
              " 'caricaturish': 53075,\n",
              " 'chitre': 70850,\n",
              " 'looonnnggg': 80846,\n",
              " 'moviewise': 58717,\n",
              " 'salomé': 80275}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOdoVxvvGFW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3f83b50e-0936-4a4c-907a-0bbdd6ccc2ad"
      },
      "source": [
        "##Let's load our embeddings\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "vector_model=KeyedVectors.load_word2vec_format(\"data/wiki-news-300d-1M.vec\", binary=False, limit=1000000)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAltzyCGGFW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#...and let's normalize\n",
        "vector_model.init_sims(replace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpY6bPP7GFW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fb2f50f-27db-4ee4-e973-68e8a596f18a"
      },
      "source": [
        "#let's create an embedding matrix\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "embedding_matrix = np.zeros((max_features, 300))\n",
        "countr = 0\n",
        "for token, idx in vocab.items():\n",
        "    \n",
        "    if idx < max_features:\n",
        "        if token in vector_model:\n",
        "            embedding_matrix[idx] = vector_model[token]\n",
        "            countr += 1\n",
        "print (countr)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMIHqg-8GFW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "8d0aa972-b6a8-42db-e161-e9481cbe4105"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "# set parameters:\n",
        "max_features = 5000\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 300\n",
        "filters = 150\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "epochs = 5\n",
        "\n",
        "print('Loading data...')\n",
        "#(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "x = Input(shape=(maxlen,))\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "\n",
        "embedding_layer = Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    weights = [embedding_matrix],\n",
        "                    input_length=maxlen)\n",
        "\n",
        "embeddings = embedding_layer(x)\n",
        "\n",
        "conv_layer = Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)\n",
        "conv_result = conv_layer(embeddings)\n",
        "pooled = (GlobalMaxPooling1D())(conv_result) \n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "out = Dense(2, activation='softmax')(pooled)\n",
        "\n",
        "model = Model(x, out)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(x_train, oh_y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, oh_y_test))\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "20000 train sequences\n",
            "5000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (20000, 400)\n",
            "x_test shape: (5000, 400)\n",
            "Build model...\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 175s 9ms/step - loss: 0.3636 - acc: 0.8354 - val_loss: 0.2672 - val_acc: 0.8900\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 171s 9ms/step - loss: 0.1738 - acc: 0.9369 - val_loss: 0.2584 - val_acc: 0.8958\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 171s 9ms/step - loss: 0.0681 - acc: 0.9821 - val_loss: 0.2800 - val_acc: 0.8968\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 171s 9ms/step - loss: 0.0172 - acc: 0.9983 - val_loss: 0.3080 - val_acc: 0.8986\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 171s 9ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.3409 - val_acc: 0.9016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa16c4d0e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZcNxBt5GFW4",
        "colab_type": "text"
      },
      "source": [
        "## Yay! It got a little better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxR9Fx2iGFW4",
        "colab_type": "text"
      },
      "source": [
        "## Good reading:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg8n2EycGFW4",
        "colab_type": "text"
      },
      "source": [
        "https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8\n",
        "http://setosa.io/ev/image-kernels/"
      ]
    }
  ]
}
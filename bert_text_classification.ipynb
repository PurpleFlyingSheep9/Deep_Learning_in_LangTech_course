{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with BERT\n",
    "\n",
    "This notebook briefly demonstrates text classification with a pretrained BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The following variables configure a few aspects of the data, model, and training process. To adapt this example to a different dataset, you'll probably want to change these to match.\n",
    "\n",
    "Note in particular that we're limiting the number of examples to just 1000 out of the original 25000 to make training faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of examples to read\n",
    "MAX_EXAMPLES = 1000\n",
    "\n",
    "# Maximum length of input sequence in tokens\n",
    "INPUT_LENGTH = 100\n",
    "\n",
    "# Number of epochs to train for\n",
    "EPOCHS = 3\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package setup\n",
    "\n",
    "We'll use [keras-bert](https://github.com/CyberZHG/keras-bert). Make sure the package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-bert in /Users/smp/Library/Python/3.7/lib/python/site-packages (0.81.0)\n",
      "Requirement already satisfied: keras-transformer>=0.30.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-bert) (0.32.0)\n",
      "Requirement already satisfied: numpy in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-bert) (1.18.1)\n",
      "Requirement already satisfied: Keras in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-bert) (2.3.1)\n",
      "Requirement already satisfied: keras-embed-sim>=0.7.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.7.0)\n",
      "Requirement already satisfied: keras-layer-normalization>=0.12.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.14.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.6.0)\n",
      "Requirement already satisfied: keras-pos-embd>=0.10.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.11.0)\n",
      "Requirement already satisfied: keras-multi-head>=0.22.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.22.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (1.14.0)\n",
      "Requirement already satisfied: pyyaml in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (5.3)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (1.4.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (1.0.8)\n",
      "Requirement already satisfied: h5py in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (2.10.0)\n",
      "Requirement already satisfied: keras-self-attention==0.41.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert) (0.41.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow configuration\n",
    "\n",
    "(This is a technical detail that is not related to the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ['TF_KERAS'] = '1'    # Required to use tensorflow.python.keras with keras-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "We'll use the familiar IMDB dataset in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘imdb_train.json’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n",
    "!wget -nc https://raw.githubusercontent.com/TurkuNLP/Deep_Learning_in_LangTech_course/master/data/imdb_train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: truncating examples from 25000 to 1000\n",
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "dict_keys(['class', 'text'])\n",
      "{'class': 'pos', 'text': \"I saw Insomniac's Nightmare not to long ago for the first time and I have to say, I really found it to be quite good. If you are a fan of Dominic Monaghan you will love it. The hole movie takes place inside his mind -or does it? The acting from everyone else is a little rushed and shaky and some of the scenes could be cut down but it works out in the end. The extras on the DVD are just as great as the film, if not greater for those Dom fans. It has tons of candid moments from the set, outtakes and a great interview with the director. Anyone who has gone through making an independent film will love to watch Tess (the director), Dom and everyone else on the very small close personal set try to bang out this little trippy creepy film. It was pretty enjoyable and I'm glad to have it in my collection.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "with open('imdb_train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "random.seed(1234)    # This makes the shuffle produce the same order every time\n",
    "random.shuffle(data)\n",
    "\n",
    "if len(data) > MAX_EXAMPLES:\n",
    "    print('Note: truncating examples from {} to {}'.format(len(data), MAX_EXAMPLES))\n",
    "    data = data[:MAX_EXAMPLES]\n",
    "\n",
    "# Look at the data\n",
    "print(type(data))\n",
    "print(type(data[0]))\n",
    "print(data[0].keys())\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I saw Insomniac's Nightmare not to long ago for the first time and I have to say, I really found it to be quite good. If you are a fan of Dominic Monaghan you will love it. The hole movie takes place inside his mind -or does it? The acting from everyone else is a little rushed and shaky and some of the scenes could be cut down but it works out in the end. The extras on the DVD are just as great as the film, if not greater for those Dom fans. It has tons of candid moments from the set, outtakes and a great interview with the director. Anyone who has gone through making an independent film will love to watch Tess (the director), Dom and everyone else on the very small close personal set try to bang out this little trippy creepy film. It was pretty enjoyable and I'm glad to have it in my collection.\n",
      "Label: pos\n"
     ]
    }
   ],
   "source": [
    "texts = [example['text'] for example in data]\n",
    "labels = [example['class'] for example in data]\n",
    "\n",
    "# Example text and label\n",
    "print('Text:', texts[0])\n",
    "print('Label:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained BERT model\n",
    "\n",
    "As training BERT from scratch generally takes days, we'll here load a pretrained model and fine-tune it for our task. URLs to download pre-trained models made available by Google are found at https://github.com/google-research/bert .\n",
    "\n",
    "`cased_L-12_H-768_A-12` is a case-sensitive BERT \"base\" model for English: 12 layers, 768-dimensional hidden state, and 12 \"heads\" for multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-15 14:57:43--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.211.16\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.211.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 404261442 (386M) [application/zip]\n",
      "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
      "\n",
      "cased_L-12_H-768_A- 100%[===================>] 385.53M  1.04MB/s    in 7m 24s  \n",
      "\n",
      "2020-04-15 15:05:08 (889 KB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n",
    "!wget -nc https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the downloaded file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  cased_L-12_H-768_A-12.zip\n",
      "   creating: cased_L-12_H-768_A-12/\n",
      "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
      "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
      "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
      "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n"
     ]
    }
   ],
   "source": [
    "# Give -n argument so that existing files aren't overwritten \n",
    "!unzip -n cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key parts to the package contents:\n",
    "\n",
    "* `vocab.txt`: plain text file listing vocabulary items\n",
    "* `bert_config.json`: model configuration in JSON format\n",
    "* `bert_model.ckpt.*`: model checkpoint data with pre-trained weights in [Tensorflow checkpoint format](https://www.tensorflow.org/guide/checkpoint)\n",
    "\n",
    "Take note of the path to these (you'll need to change this if you pick a different model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_path = 'cased_L-12_H-768_A-12/vocab.txt'\n",
    "bert_config_path = 'cased_L-12_H-768_A-12/bert_config.json'\n",
    "bert_checkpoint_path = 'cased_L-12_H-768_A-12/bert_model.ckpt'    # suffixes not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also take note if the model we downloaded was a case-sensitive (cased) or not. (This must match the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_is_cased = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT vocabulary\n",
    "\n",
    "This is just a plain text file with one vocabulary item per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', 'щ', '吉', 'told', 'space', 'operations', 'proposed', 'Oxford', 'showing', 'domestic', 'mountains', 'commission', 'voices', 'associate', 'hills', 'Guide', 'relaxed', 'Page', 'Heights', 'singers', 'Interior', 'considers', 'facilitate', 'shouting', '1826', 'constitute', 'alter', 'clip', 'Into', 'Memory', 'ballad', 'Owens', 'Langdon', 'aquatic', 'stereo', 'Cass', 'Shock', '195', '##tec', '##sonic', 'attested', '##rdes', '1840s', '##90', 'Guys', '##rien', 'Munro', 'Ursula', 'mesh', 'diplomacy', 'Newmarket', '##oughs', 'synthesizers', 'Drugs', 'monstrous', '##ynamic', 'troll', '##ٹ']\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "with open(bert_vocab_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vocab.append(line.rstrip('\\n'))\n",
    "\n",
    "\n",
    "# print a list with every 500th vocabulary item\n",
    "print(vocab[0::500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_probs_dropout_prob': 0.1,\n",
      " 'hidden_act': 'gelu',\n",
      " 'hidden_dropout_prob': 0.1,\n",
      " 'hidden_size': 768,\n",
      " 'initializer_range': 0.02,\n",
      " 'intermediate_size': 3072,\n",
      " 'max_position_embeddings': 512,\n",
      " 'num_attention_heads': 12,\n",
      " 'num_hidden_layers': 12,\n",
      " 'type_vocab_size': 2,\n",
      " 'vocab_size': 28996}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint    # pretty-printer for output\n",
    "\n",
    "\n",
    "with open(bert_config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "\n",
    "# Print configuration contents\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'##ein': 20309,\n",
      " '##nce': 3633,\n",
      " '2014': 1387,\n",
      " 'Dennis': 6277,\n",
      " 'Jam': 13263,\n",
      " 'coffin': 16638,\n",
      " 'dense': 9613,\n",
      " 'frequencies': 13714,\n",
      " 'recovering': 14299,\n",
      " 'れ': 929}\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from vocabulary items to their indices in the vocabulary\n",
    "token_dict = { v: i for i, v in enumerate(vocab) }\n",
    "\n",
    "\n",
    "# Print some random examples of the mapping\n",
    "pprint(dict(random.choices(list(token_dict.items()), k=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the keras-bert `Tokenizer` for BERT tokenization. The implementation supports\n",
    "\n",
    "* (Optional) lowercasing: `Hello` → `hello`\n",
    "* Basic tokenization: `Hello!` → `Hello` `!`, `multi-part` → `multi` `-` `part`\n",
    "* Wordpiece tokenization: `comprehensively` → `comprehensive` `##ly`\n",
    "* Adding special tokens: `Sentence`  → `[CLS]` `Sentence` `[SEP]`\n",
    "* Mapping to integer indices\n",
    "* Generating segment sequence\n",
    "* (Optional) padding and truncation to length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: Hello BERT!\n",
      "Tokenized: ['[CLS]', 'Hello', 'B', '##ER', '##T', '!', '[SEP]']\n",
      "Encoded: [101, 8667, 139, 9637, 1942, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded: Hello B ##ER ##T !\n",
      "\n",
      "Original string: Unknown: 你\n",
      "Tokenized: ['[CLS]', 'Unknown', ':', '你', '[SEP]']\n",
      "Encoded: [101, 16285, 131, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded: Unknown : [UNK]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(token_dict, cased=model_is_cased)\n",
    "\n",
    "\n",
    "# Let's test that out\n",
    "for s in ['Hello BERT!', 'Unknown: 你']:\n",
    "    print('Original string:', s)\n",
    "    print('Tokenized:', tokenizer.tokenize(s))\n",
    "    indices, segments = tokenizer.encode(s, max_len=20)    # max_len for padding and truncation\n",
    "    print('Encoded:', indices)\n",
    "    print('Segments:', segments)\n",
    "    print('Decoded:', ' '.join(tokenizer.decode(indices)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data\n",
    "\n",
    "We'll use the familiar `LabelEncoder` for labels and the keras-bert `Tokenizer` for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 2\n",
      "<class 'list'> ['pos', 'neg', 'pos', 'pos', 'pos', 'neg', 'neg', 'pos', 'neg', 'pos']\n",
      "<class 'numpy.ndarray'> [1 0 1 1 1 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()    # Turns class labels into integers\n",
    "Y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Take note of how many unique labels there are in the data\n",
    "num_labels = len(set(Y))\n",
    "\n",
    "\n",
    "# Print out some examples\n",
    "print('Number of unique labels:', num_labels)\n",
    "print(type(labels), labels[:10])\n",
    "print(type(Y), Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep token indices and segment ids in separate lists and store as numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token indices:\n",
      "[[  101   146  1486  1130  7301  1306  5813  1665   112   188 26247  1136\n",
      "   1106  1263  2403  1111  1103  1148  1159  1105   146  1138  1106  1474\n",
      "    117   146  1541  1276  1122  1106  1129  2385  1363   119  1409  1128\n",
      "   1132   170  5442  1104 11288 15706 23136  1128  1209  1567  1122   119\n",
      "   1109  4569  2523  2274  1282  1656  1117  1713   118  1137  1674  1122\n",
      "    136  1109  3176  1121  2490  1950  1110   170  1376  6169  1105 16591\n",
      "   1105  1199  1104  1103  4429  1180  1129  2195  1205  1133  1122  1759\n",
      "   1149  1107  1103  1322   119  1109  3908  1116  1113  1103  4173  1132\n",
      "   1198  1112  1632   102]\n",
      " [  101  1109  1273  1110   170  2436  1104   172 26567   112   188  1113\n",
      "   1198  1164  1625  1149  1175   119  1135  1144  1185  2817 20748   117\n",
      "   1185  2513   117  1185  1842  3802   119   156 17162 15792  1863  1110\n",
      "   2873  1166  1103  1499  1105 17000  2340  2624  1110 13504  1105 22052\n",
      "   2285   119  1188  2523  1169   112   189  9345  1103 23483  1104  1543\n",
      "   3362  1187  1664  5903   119  4081  1353  3670  1104  1734  3587  2411\n",
      "   1154   170 12628   117  8362  9380  2050  6202 17693  8032  1115  1110\n",
      "   1177  4701  1104  1126  3176  1705 20762   119  2777  1175  1110  1185\n",
      "   9556  1106  1142   102]]\n",
      "Decoded:\n",
      "['I', 'saw', 'In', '##so', '##m', '##nia', '##c', \"'\", 's', 'Nightmare', 'not', 'to', 'long', 'ago', 'for', 'the', 'first', 'time', 'and', 'I', 'have', 'to', 'say', ',', 'I', 'really', 'found', 'it', 'to', 'be', 'quite', 'good', '.', 'If', 'you', 'are', 'a', 'fan', 'of', 'Dominic', 'Mona', '##ghan', 'you', 'will', 'love', 'it', '.', 'The', 'hole', 'movie', 'takes', 'place', 'inside', 'his', 'mind', '-', 'or', 'does', 'it', '?', 'The', 'acting', 'from', 'everyone', 'else', 'is', 'a', 'little', 'rushed', 'and', 'shaky', 'and', 'some', 'of', 'the', 'scenes', 'could', 'be', 'cut', 'down', 'but', 'it', 'works', 'out', 'in', 'the', 'end', '.', 'The', 'extra', '##s', 'on', 'the', 'DVD', 'are', 'just', 'as', 'great']\n",
      "['The', 'film', 'is', 'a', 'collection', 'of', 'c', '##liche', \"'\", 's', 'on', 'just', 'about', 'anything', 'out', 'there', '.', 'It', 'has', 'no', 'focus', 'whatsoever', ',', 'no', 'goals', ',', 'no', 'real', 'message', '.', 'S', '##ym', '##bol', '##ism', 'is', 'pushed', 'over', 'the', 'top', 'and', 'stereo', '##ty', '##ping', 'is', 'abundant', 'and', 'outrage', '##ous', '.', 'This', 'movie', 'can', \"'\", 't', 'resist', 'the', 'temptation', 'of', 'making', 'drama', 'where', 'non', 'exists', '.', 'Every', 'small', 'exchange', 'of', 'words', 'turns', 'immediately', 'into', 'a', 'lengthy', ',', 'un', '##ju', '##st', '##ified', 'dial', '##og', 'that', 'is', 'so', 'typical', 'of', 'an', 'acting', 'class', 'rehearsal', '.', 'Where', 'there', 'is', 'no', 'substance', 'to', 'this']\n",
      "Segment ids:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "token_indices, segment_ids = [], []\n",
    "for text in texts:\n",
    "    tid, sid = tokenizer.encode(text, max_len=INPUT_LENGTH)\n",
    "    token_indices.append(tid)\n",
    "    segment_ids.append(sid)\n",
    "\n",
    "# Format input as list of two numpy arrays\n",
    "X = [np.array(token_indices), np.array(segment_ids)]\n",
    "\n",
    "\n",
    "# Print some examples\n",
    "print('Token indices:')\n",
    "print(X[0][:2])\n",
    "print('Decoded:')\n",
    "for i in X[0][:2]:\n",
    "    print(tokenizer.decode(list(i)))\n",
    "print('Segment ids:')\n",
    "print(X[1][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained BERT model\n",
    "\n",
    "We'll use the keras-bert function `load_trained_model_from_checkpoint` to load the model from the checkpoint we downloaded earlier.\n",
    "\n",
    "Explanation for a few parameters from keras-bert documentation:\n",
    "\n",
    "* `training`: If `training`, the whole model will be returned. Otherwise, the MLM [masked language modeling] and NSP [next sentence prediction] parts will be ignored.\n",
    "* `trainable`: Whether the model is trainable. The default value is the same with `training`.\n",
    "\n",
    "We don't need the masked language modeling or next sentence prediction parts (these are primarily for pretraining), so we'll use `training=False` but `trainable=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "\n",
    "\n",
    "pretrained_model = load_trained_model_from_checkpoint(\n",
    "    config_file = bert_config_path,\n",
    "    checkpoint_file = bert_checkpoint_path,\n",
    "    training = False,\n",
    "    trainable = True,\n",
    "    seq_len = INPUT_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a bit of a look at that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Input-Token_4:0' shape=(?, 100) dtype=float32>,\n",
       " <tf.Tensor 'Input-Segment_4:0' shape=(?, 100) dtype=float32>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Encoder-12-FeedForward-Norm_4/add_1:0' shape=(?, 100, 768) dtype=float32>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 100, 768), ( 22268928    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 100, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 100, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 100, 768)     76800       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 100, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 100, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 100, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 100, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 100, 768)     0           Encoder-10-MultiHeadSelfAttention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 100, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 100, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 100, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 100, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 100, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 100, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 100, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 100, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 100, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "==================================================================================================\n",
      "Total params: 107,403,264\n",
      "Trainable params: 107,403,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a regular Keras model. In Keras, models behave very much like layers, so we're able to wrap this in our own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build classification model\n",
    "\n",
    "We'll make a *very* simple model for text classification: just attach a dense layer to the output for the special `[CLS]` token, and connect the model inputs to the BERT model inputs.\n",
    "\n",
    "First, let's find the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'Encoder-12-FeedForward-Norm_4/add_1:0' shape=(?, 100, 768) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those dimensions are (minibatch-size, sequence-length, hidden-dim).\n",
    "\n",
    "We'll just need the first sequence position across all elements in the initial (minibatch) dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_4:0\", shape=(?, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "bert_out = pretrained_model.outputs[0][:,0]\n",
    "\n",
    "print(bert_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "out = Dense(num_labels, activation='softmax')(bert_out)\n",
    "model = Model(\n",
    "    inputs=pretrained_model.inputs,\n",
    "    outputs=[out]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create optimizer\n",
    "\n",
    "BERT is pretrained with an Adam optimizer with warmup and regularization using weight decay. We won't go into detail on these optimizer settings, but will instead largely copy the BERT pretraining parameters.\n",
    "\n",
    "(If you're interested in tuning the training process, trying different values of `learning_rate` is a good place to start!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import calc_train_steps, AdamWarmup\n",
    "\n",
    "\n",
    "# Learning rate setting\n",
    "learning_rate = 0.00005\n",
    "\n",
    "# Calculate the number of steps for warmup\n",
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=len(texts),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(\n",
    "    total_steps,\n",
    "    warmup_steps,\n",
    "    lr=learning_rate,\n",
    "    epsilon=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "The model is compiled and trained normally. As usual, we'll use `sparse_categorical_crossentropy` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: this will take some time unless you're running with GPU acceleration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/smp/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/3\n",
      "900/900 [==============================] - 1261s 1s/sample - loss: 0.6622 - sparse_categorical_accuracy: 0.6478 - val_loss: 0.7608 - val_sparse_categorical_accuracy: 0.5500\n",
      "Epoch 2/3\n",
      "900/900 [==============================] - 1178s 1s/sample - loss: 0.3537 - sparse_categorical_accuracy: 0.8444 - val_loss: 0.3066 - val_sparse_categorical_accuracy: 0.8900\n",
      "Epoch 3/3\n",
      "900/900 [==============================] - 1181s 1s/sample - loss: 0.0666 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.4433 - val_sparse_categorical_accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'],label=\"Training set accuracy\")\n",
    "    plt.plot(history.history['val_sparse_categorical_accuracy'],label=\"Validation set accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely overfit by the end of the third epoch; training set accuracy goes to 98%. The best dev set result is for the second epoch, at 89%. Not a bad result for such a small sample of the data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Named entity recognition</h2>\n",
    "<br>\n",
    "Sequence labelling: input word sequence, output tag sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ner.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load CoNLL'03 data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the CoNLL'03 dataset. The data in their original format can be found here https://github.com/glample/tagger .<br><br>\n",
    "\n",
    "In this demo, the dataset has already been converted to json files. The converted data are found here under the file names `ner_train.json` and `ner_test.json`: https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/tree/master/data The code used for convertion can be found here: https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course/blob/master/read_ner.ipynb <br><br>\n",
    "\n",
    "The data is divided into separate sentences and has also been tokenized already. You don't have to do any preprocessing. The produced json files contain a single dictionary for each sentence. The dictionary has a list of tokens and the corresponding list of labels, e.g.: { \"text\": [ \"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\" ], \"tags\": [ \"I-ORG\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\" ] },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "dict_keys(['text', 'tags'])\n",
      "{'text': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'tags': ['I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"data/ner-conll03-en-train.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    \n",
    "# Look at the data\n",
    "print(type(data))\n",
    "print(type(data[0]))\n",
    "print(data[0].keys())\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be making use of cross-sentence information, so we can shuffle the data on the sentence level (**not** the token level). Let's also separate the texts and tags from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['4', '-', 'Kimiko', 'Date', '(', 'Japan', ')', 'beat', '2', '-', 'Conchita', 'Martinez', '(', 'Spain', ')', '6-2', '7-5', '.']\n",
      "Label: ['O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'O', 'I-PER', 'I-PER', 'O', 'I-LOC', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "\n",
    "random.seed(123)    # This makes the shuffle produce the same order every time\n",
    "random.shuffle(data)\n",
    "\n",
    "train_texts = [example[\"text\"] for example in data]\n",
    "train_labels = [example[\"tags\"] for example in data]\n",
    "\n",
    "# Example text and labels\n",
    "print('Text:', train_texts[0])\n",
    "print('Label:', train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the development (validation) data. A separate validation set involving different documents than the train data is used as splitting one set of documents into training and validation data would result in overly optimistic evaluation results (repeated names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ner-conll03-en-dev.json\") as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "validation_texts = [example[\"text\"] for example in validation_data]\n",
    "validation_labels = [example[\"tags\"] for example in validation_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained embeddings\n",
    "\n",
    "We'll initialize our embeddings with word vectors pretrained on the English Wikipedia.\n",
    "\n",
    "(Don't worry if the download, unpacking and loading take some time, this is a fair amount of data.)\n",
    "\n",
    "Download embeddings from https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-02 09:34:32--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650.22M  1.15MB/s    in 13m 23s \n",
      "\n",
      "2020-04-02 09:47:56 (829 KB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use gensim to read the embedding model.\n",
    "\n",
    "Note that we are using a vocabulary size of only 50,000 so that the model trains faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words from embedding model: 50000\n",
      "First 50 words: [',', 'the', '.', 'and', 'of', 'to', 'in', 'a', '\"', ':', ')', 'that', '(', 'is', 'for', 'on', '*', 'with', 'as', 'it', 'The', 'or', 'was', \"'\", \"'s\", 'by', 'from', 'at', 'I', 'this', 'you', '/', 'are', '=', 'not', '-', 'have', '?', 'be', 'which', ';', 'all', 'his', 'has', 'one', 'their', 'about', 'but', 'an', '|']\n",
      "Before normalization: [-0.0234 -0.0268 -0.0838  0.0386 -0.0321  0.0628  0.0281 -0.0252  0.0269\n",
      " -0.0063]\n",
      "After normalization: [-0.0163762  -0.01875564 -0.05864638  0.02701372 -0.02246478  0.04394979\n",
      "  0.01966543 -0.0176359   0.01882563 -0.00440898]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_model = KeyedVectors.load_word2vec_format(\"wiki-news-300d-1M.vec\", binary=False, limit=50000)\n",
    "\n",
    "\n",
    "# sort based on the index to make sure they are in the correct order\n",
    "words = [k for k, v in sorted(vector_model.vocab.items(), key=lambda x: x[1].index)]\n",
    "print(\"Words from embedding model:\", len(words))\n",
    "print(\"First 50 words:\", words[:50])\n",
    "\n",
    "# Normalize the vectors to unit length\n",
    "print(\"Before normalization:\", vector_model.get_vector(\"in\")[:10])\n",
    "vector_model.init_sims(replace=True)\n",
    "print(\"After normalization:\", vector_model.get_vector(\"in\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary: 50002\n",
      "Found pretrained vectors for 50000 words.\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary mappings\n",
    "\n",
    "# Zero is used for padding in Keras, prevent using it for a normal word.\n",
    "# Also reserve an index for out-of-vocabulary items.\n",
    "vocabulary={\n",
    "    \"<PAD>\": 0,\n",
    "    \"<OOV>\": 1\n",
    "}\n",
    "\n",
    "for word in words: # These are words from the word2vec model\n",
    "    vocabulary.setdefault(word, len(vocabulary))\n",
    "\n",
    "print(\"Words in vocabulary:\",len(vocabulary))\n",
    "inv_vocabulary = { value: key for key, value in vocabulary.items() } # invert the dictionary\n",
    "\n",
    "\n",
    "# Embedding matrix\n",
    "def load_pretrained_embeddings(vocab, embedding_model):\n",
    "    \"\"\" vocab: vocabulary from our data vectorizer, embedding_model: model loaded with gensim \"\"\"\n",
    "    pretrained_embeddings = numpy.random.uniform(low=-0.05, high=0.05, size=(len(vocab)-1,embedding_model.vectors.shape[1]))\n",
    "    pretrained_embeddings = numpy.vstack((numpy.zeros(shape=(1,embedding_model.vectors.shape[1])), pretrained_embeddings))\n",
    "    found=0\n",
    "    for word,idx in vocab.items():\n",
    "        if word in embedding_model.vocab:\n",
    "            pretrained_embeddings[idx]=embedding_model.get_vector(word)\n",
    "            found+=1\n",
    "            \n",
    "    print(\"Found pretrained vectors for {found} words.\".format(found=found))\n",
    "    return pretrained_embeddings\n",
    "\n",
    "pretrained=load_pretrained_embeddings(vocabulary, vector_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocess data</h3>\n",
    "<br>\n",
    "Both the text and the labels are strings; we'll need to convert the labels into integers and the text into an appropriate format for RNN training.<br>\n",
    "<h4>Labels</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-LOC': 1, 'I-ORG': 0, 'I-PER': 3, 'O': 2}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Label mappings\n",
    "# 1) gather a set of unique labels\n",
    "label_set = set()\n",
    "for sentence_labels in train_labels: #loops over sentences \n",
    "    for label in sentence_labels: #loops over labels in one sentence\n",
    "        label_set.add(label)\n",
    "\n",
    "# 2) index these\n",
    "label_map = {}\n",
    "for index, label in enumerate(label_set):\n",
    "    label_map[label]=index\n",
    "    \n",
    "pprint(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 3, 3, 2, 1, 2, 2, 2, 2, 3, 3, 2, 1, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# vectorize the labels\n",
    "def label_vectorizer(labels,label_map):\n",
    "    vectorized_labels = []\n",
    "    for label in labels:\n",
    "        vectorized_example_label = []\n",
    "        for token in label:\n",
    "            vectorized_example_label.append(label_map[token])\n",
    "        vectorized_labels.append(vectorized_example_label)\n",
    "    vectorized_labels = numpy.array(vectorized_labels)\n",
    "    return vectorized_labels\n",
    "        \n",
    "\n",
    "vectorized_labels = label_vectorizer(train_labels, label_map)\n",
    "validation_vectorized_labels = label_vectorizer(validation_labels, label_map)\n",
    "\n",
    "pprint(vectorized_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Texts</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes the vectorization of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '-', 'Kimiko', 'Date', '(', 'Japan', ')', 'beat', '2', '-', 'Conchita', 'Martinez', '(', 'Spain', ')', '6-2', '7-5', '.']\n",
      "[225, 37, 1, 9859, 14, 799, 12, 2270, 142, 37, 1, 10290, 14, 1366, 12, 13775, 21996, 4]\n",
      "['4', '-', '<OOV>', 'Date', '(', 'Japan', ')', 'beat', '2', '-', '<OOV>', 'Martinez', '(', 'Spain', ')', '6-2', '7-5', '.']\n"
     ]
    }
   ],
   "source": [
    "def text_vectorizer(vocab, texts):\n",
    "    vectorized_data = [] # turn text into numbers based on our vocabulary mapping\n",
    "    sentence_lengths = [] # Number of tokens in each sentence\n",
    "    \n",
    "    for i, one_example in enumerate(texts):\n",
    "        vectorized_example = []\n",
    "        for word in one_example:\n",
    "            vectorized_example.append(vocab.get(word, 1)) # 1 is our index for out-of-vocabulary tokens\n",
    "\n",
    "        vectorized_data.append(vectorized_example)     \n",
    "        sentence_lengths.append(len(one_example))\n",
    "        \n",
    "    vectorized_data = numpy.array(vectorized_data) # turn python list into numpy array\n",
    "    \n",
    "    return vectorized_data, sentence_lengths\n",
    "\n",
    "vectorized_data, lengths=text_vectorizer(vocabulary, train_texts)\n",
    "validation_vectorized_data, validation_lengths=text_vectorizer(vocabulary, validation_texts)\n",
    "\n",
    "print(train_texts[0])\n",
    "print(vectorized_data[0])\n",
    "print([inv_vocabulary[i] for i in vectorized_data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are a lot of out-of-vocabulary tokens because of the small vocabulary we use. The number of `<OOV>` tokens can be decreased by using a larger vocabulary.<br><br>\n",
    "We have to pad the sequences so that all of them are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: (14041,)\n",
      "New shape: (14041, 113)\n",
      "First example: [  225    37     1  9859    14   799    12  2270   142    37     1 10290\n",
      "    14  1366    12 13775 21996     4     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n",
      "Padded labels shape: (14041, 113, 1)\n",
      "{'I-LOC': 1, 'I-ORG': 0, 'I-PER': 3, 'O': 2}\n",
      "First example labels:\n",
      "array([[2],\n",
      "       [2],\n",
      "       [3],\n",
      "       [3],\n",
      "       [2],\n",
      "       [1],\n",
      "       [2],\n",
      "       [2],\n",
      "       [2],\n",
      "       [2],\n",
      "       [3],\n",
      "       [3],\n",
      "       [2],\n",
      "       [1],\n",
      "       [2],\n",
      "       [2],\n",
      "       [2],\n",
      "       [2],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0]], dtype=int32)\n",
      "First weight vector: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "### Only needed when sharing GPUs, this avoids blocking the whole GPU, you don't need this stuff\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "### ---end of weird stuff\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "print(\"Old shape:\", vectorized_data.shape)\n",
    "vectorized_data_padded=pad_sequences(vectorized_data, padding='post', maxlen=max(lengths))\n",
    "print(\"New shape:\", vectorized_data_padded.shape)\n",
    "print(\"First example:\", vectorized_data_padded[0])\n",
    "\n",
    "# Even with the sparse output format, the shape has to be similar to the one-hot encoding\n",
    "vectorized_labels_padded=numpy.expand_dims(pad_sequences(vectorized_labels, padding='post', maxlen=max(lengths)), -1)\n",
    "print(\"Padded labels shape:\", vectorized_labels_padded.shape)\n",
    "pprint(label_map)\n",
    "print(\"First example labels:\")\n",
    "pprint(vectorized_labels_padded[0])\n",
    "\n",
    "weights = numpy.copy(vectorized_data_padded)\n",
    "weights[weights > 0] = 1\n",
    "print(\"First weight vector:\", weights[0])\n",
    "\n",
    "# Same stuff for the validation data\n",
    "validation_vectorized_data_padded=pad_sequences(validation_vectorized_data, padding='post', maxlen=max(lengths))\n",
    "validation_vectorized_labels_padded=numpy.expand_dims(pad_sequences(validation_vectorized_labels, padding='post',maxlen=max(lengths)), -1)\n",
    "validation_weights = numpy.copy(validation_vectorized_data_padded)\n",
    "validation_weights[validation_weights > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Evaluation function</h4>\n",
    "<br>\n",
    "The entities can continue beyond one token so the model has to evaluate predictions based on that. We write a function that converts the sequence of tokens into entities. We then write another function that calculates precision, recall, and f-score based on the predictions. We put these functions into a custom callback which we use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "\n",
    "def _convert_to_entities(input_sequence):\n",
    "    \"\"\"\n",
    "    Reads a sequence of tags and converts them into a set of entities.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    previous_tag = label_map['O']\n",
    "    for i, tag in enumerate(input_sequence):\n",
    "        if tag != previous_tag and tag != label_map['O']: # New entity starts\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "            current_entity.append((tag, i))\n",
    "        elif tag == label_map['O']: # Entity has ended\n",
    "            if len(current_entity) > 0:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = []\n",
    "        elif tag == previous_tag: # Current entity continues\n",
    "            current_entity.append((tag, i))\n",
    "        previous_tag = tag\n",
    "    \n",
    "    # Add the last entity to our entity list if the sentences ends with an entity\n",
    "    if len(current_entity) > 0:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    entity_offsets = set()\n",
    "    \n",
    "    for e in entities:\n",
    "        entity_offsets.add((e[0][0], e[0][1], e[-1][1]+1))\n",
    "    return entity_offsets\n",
    "\n",
    "\n",
    "def _entity_level_PRF(predictions, gold, lengths):\n",
    "    pred_entities = [_convert_to_entities(labels[:lengths[i]]) for i, labels in enumerate(predictions)]\n",
    "    gold_entities = [_convert_to_entities(labels[:lengths[i], 0]) for i, labels in enumerate(gold)]\n",
    "    \n",
    "    tp = sum([len(pe.intersection(gold_entities[i])) for i, pe in enumerate(pred_entities)])\n",
    "    pred_count = sum([len(e) for e in pred_entities])\n",
    "    \n",
    "    try:\n",
    "        precision = tp / pred_count # tp / (tp+np)\n",
    "        recall = tp / sum([len(e) for e in gold_entities])\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    except Exception as e:\n",
    "        precision, recall, fscore = 0.0, 0.0, 0.0\n",
    "    print('\\nPrecision/Recall/F-score: %s / %s / %s' % (precision, recall, fscore))\n",
    "    return precision, recall, fscore             \n",
    "\n",
    "\n",
    "def evaluate(predictions, gold, lengths):\n",
    "    precision, recall, fscore = _entity_level_PRF(predictions, gold, lengths)\n",
    "    return precision, recall, fscore\n",
    "\n",
    "\n",
    "class EvaluateEntities(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.fscore = []\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        pred = numpy.argmax(self.model.predict(validation_vectorized_data_padded), axis=-1)\n",
    "        evaluation_parameters=evaluate(pred, validation_vectorized_labels_padded, validation_lengths)\n",
    "        self.precision.append(evaluation_parameters[0])\n",
    "        self.recall.append(evaluation_parameters[1])\n",
    "        self.fscore.append(evaluation_parameters[2])\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predicting word labels with word embeddings</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into RNNs, we first build a simpler model without any recurrent layers:\n",
    "<br>\n",
    "- input: sequence of `sequence_length` integers corresponding to words<br>\n",
    "- embedding: pretrained mapping from integers to `vector_size`-dimensional vectors<br>\n",
    "- hidden: `hidden_size`-dimensional fully connected layer with relu activation<br>\n",
    "- output: `class_count`-dimensional fully connected layer with softmax activation<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/smp/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Activation, TimeDistributed\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "hidden_size = 100\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "\n",
    "def build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=True, trainable=False, weights=[pretrained])(inp)\n",
    "    hidden = TimeDistributed(Dense(hidden_size, activation=\"relu\"))(embeddings) # We change this activation function\n",
    "    outp = TimeDistributed(Dense(class_count, activation=\"softmax\"))(hidden)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "\n",
    "model = build_model(example_count, sequence_len, class_count, hidden_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 113, 100)          30100     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 113, 4)            404       \n",
      "=================================================================\n",
      "Total params: 15,031,104\n",
      "Trainable params: 30,504\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the model</h3>\n",
    "<br>\n",
    "We'll then compile and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/smp/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/smp/Library/Python/3.7/lib/python/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      " - 7s - loss: 0.0696\n",
      "\n",
      "Precision/Recall/F-score: 0.5664425232148576 / 0.35239043824701194 / 0.43448360555077975\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.0294\n",
      "\n",
      "Precision/Recall/F-score: 0.5628946090335114 / 0.4617529880478088 / 0.507332020135697\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.0257\n",
      "\n",
      "Precision/Recall/F-score: 0.5690555291884195 / 0.47768924302788845 / 0.5193848819579814\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.0247\n",
      "\n",
      "Precision/Recall/F-score: 0.575212866603595 / 0.48446215139442234 / 0.5259515570934256\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.0241\n",
      "\n",
      "Precision/Recall/F-score: 0.5727568572756857 / 0.49083665338645416 / 0.528641922334263\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.0238\n",
      "\n",
      "Precision/Recall/F-score: 0.5852567121997174 / 0.4950199203187251 / 0.5363695229872653\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.0235\n",
      "\n",
      "Precision/Recall/F-score: 0.5926100258884444 / 0.501593625498008 / 0.5433164311144676\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.0233\n",
      "\n",
      "Precision/Recall/F-score: 0.5948356807511737 / 0.5047808764940239 / 0.5461206896551724\n",
      "Epoch 9/10\n",
      " - 9s - loss: 0.0231\n",
      "\n",
      "Precision/Recall/F-score: 0.5965735742783385 / 0.5063745019920318 / 0.5477857989440793\n",
      "Epoch 10/10\n",
      " - 8s - loss: 0.0230\n",
      "\n",
      "Precision/Recall/F-score: 0.6024464831804281 / 0.5101593625498008 / 0.5524754611153059\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=0.001) # define the learning rate\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "vanilla_hist=model.fit(\n",
    "    vectorized_data_padded,\n",
    "    vectorized_labels_padded,\n",
    "    sample_weight=weights,\n",
    "    batch_size=100,\n",
    "    verbose=2,\n",
    "    epochs=10,\n",
    "    callbacks=[evaluation_function]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the f-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History: [0.43448360555077975, 0.507332020135697, 0.5193848819579814, 0.5259515570934256, 0.528641922334263, 0.5363695229872653, 0.5433164311144676, 0.5461206896551724, 0.5477857989440793, 0.5524754611153059]\n",
      "Highest f-score: 0.5524754611153059\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(fscores):\n",
    "    print(\"History:\", fscores)\n",
    "    print(\"Highest f-score:\", max(fscores))\n",
    "    plt.plot(fscores)\n",
    "    plt.legend(loc='lower center', borderaxespad=0.)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to try yourself\n",
    "\n",
    "Try playing around with the neural network. You could, for example, try\n",
    "\n",
    "- Different activation functions\n",
    "- Altering the learning rate\n",
    "- Use different optimizers\n",
    "- Adjusting the vocabulary size of the embeddings\n",
    "\n",
    "Activation functions and optimizers supported by Keras can be found here: https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Including the context</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then include the context by defining and building an RNN model which has the following layers:\n",
    "<br>\n",
    "- input: sequence of `sequence_length` integers corresponding to words\n",
    "- embedding: pretrained mapping from integers to `vector_size`-dimensional vectors\n",
    "- rnn: recurrent neural network, specifically LSTM, with `rnn_size`-dimensional state\n",
    "- output: `class_count`-dimensional fully connected layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "example_count, sequence_len = vectorized_data_padded.shape\n",
    "class_count = len(label_set)\n",
    "rnn_size = 100\n",
    "\n",
    "vector_size= pretrained.shape[1]\n",
    "\n",
    "\n",
    "def build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained):\n",
    "    inp=Input(shape=(sequence_len,))\n",
    "    embeddings=Embedding(len(vocabulary), vector_size, mask_zero=False, trainable=False, weights=[pretrained])(inp)\n",
    "    rnn = LSTM(rnn_size, activation='relu', return_sequences=True)(embeddings)\n",
    "    outp=Dense(class_count, activation=\"softmax\")(rnn)\n",
    "    return Model(inputs=[inp], outputs=[outp])\n",
    "\n",
    "\n",
    "rnn_model = build_rnn_model(example_count, sequence_len, class_count, rnn_size, vocabulary, vector_size, pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 113)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 113, 300)          15000600  \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 113, 100)          30100     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 113, 4)            404       \n",
      "=================================================================\n",
      "Total params: 15,031,104\n",
      "Trainable params: 30,504\n",
      "Non-trainable params: 15,000,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train an RNN model</h3>\n",
    "<br>\n",
    "We compile and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 81s - loss: 0.0795\n",
      "\n",
      "Precision/Recall/F-score: 0.0 / 0.0 / 0.0\n",
      "Epoch 2/10\n",
      " - 73s - loss: 0.0441\n",
      "\n",
      "Precision/Recall/F-score: 0.5872093023255814 / 0.44262948207171315 / 0.5047705588368925\n",
      "Epoch 3/10\n",
      " - 80s - loss: 0.0261\n",
      "\n",
      "Precision/Recall/F-score: 0.6366378443595415 / 0.6306772908366534 / 0.6336435504853397\n",
      "Epoch 4/10\n",
      " - 83s - loss: 0.0207\n",
      "\n",
      "Precision/Recall/F-score: 0.7183456561922366 / 0.6193227091633466 / 0.6651690201112538\n",
      "Epoch 5/10\n",
      " - 89s - loss: 0.0184\n",
      "\n",
      "Precision/Recall/F-score: 0.7392360344446898 / 0.6669322709163347 / 0.7012252591894438\n",
      "Epoch 6/10\n",
      " - 72s - loss: 0.0170\n",
      "\n",
      "Precision/Recall/F-score: 0.7291534726620224 / 0.6880478087649402 / 0.7080045095828635\n",
      "Epoch 7/10\n",
      " - 71s - loss: 0.0163\n",
      "\n",
      "Precision/Recall/F-score: 0.7565017261219793 / 0.654780876494024 / 0.7019754404698345\n",
      "Epoch 8/10\n",
      " - 71s - loss: 0.0154\n",
      "\n",
      "Precision/Recall/F-score: 0.7517543859649123 / 0.6828685258964143 / 0.7156576200417536\n",
      "Epoch 9/10\n",
      " - 72s - loss: 0.0147\n",
      "\n",
      "Precision/Recall/F-score: 0.7608360836083609 / 0.6888446215139442 / 0.7230527966544694\n",
      "Epoch 10/10\n",
      " - 69s - loss: 0.0143\n",
      "\n",
      "Precision/Recall/F-score: 0.7516240796881767 / 0.6914342629482072 / 0.7202739157501556\n"
     ]
    }
   ],
   "source": [
    "optimizer=Adam(lr=0.001) # define the learning rate\n",
    "rnn_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", sample_weight_mode='temporal')\n",
    "\n",
    "evaluation_function=EvaluateEntities()\n",
    "\n",
    "# train\n",
    "rnn_hist=rnn_model.fit(vectorized_data_padded,vectorized_labels_padded, sample_weight=weights, batch_size=100,verbose=2,epochs=10, callbacks=[evaluation_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the f-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History: [0.0, 0.6104056635577032, 0.6603936837551373, 0.691867592970985, 0.7107455226988755, 0.7138692211578508, 0.7198280274337191, 0.7117088269826021, 0.7273465246067986, 0.7256581696615126]\n",
      "Highest f-score: 0.7273465246067986\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHINJREFUeJzt3WtwXOd93/Hvf3dxBwheAFIULiRAkZYhkYpkhJLs1HFsy5Vql8rEdkupSeM2DacdK3HjNI2cpkqrvEnSjhO/0HTCuu5kWtOKK7spk7BRM7U7vYxFkbrt8mLJFCgSu6Qk8IIFcd/Lvy92AS5gkFiSCx7s2d9nBrPn8mD3jyXx24PnOec85u6IiEi4RIIuQEREKk/hLiISQgp3EZEQUriLiISQwl1EJIQU7iIiIaRwFxEJIYW7iEgIKdxFREIoFtQLd3R0+NatW4N6eRGRqvTKK69ccPfO5doFFu5bt27l6NGjQb28iEhVMrMz5bRTt4yISAgp3EVEQkjhLiISQgp3EZEQUriLiISQwl1EJIQU7iIiIRTYee4iIivJ3RkZnyF5eYrhS5OkRqeYyeSJRuzql9nC9ZLtsagRMSMWMSKRhY9R+/FtkeL3XPc5IxEiEWisi1IXXdlja4W7iFQld+fC+CzJy5MkL08VQnx+eZLU5Slmsvmgy1zS7/7svfzCQ1tW9DUU7iJVKJ93JjM5JmeyTMzmmJjJMll8nJjNMjmTY2I2Sy7vrGmsY01THWuaYrQ31c1/tTbEMLOgf5RrcncuTcwyXAzr5ILHwvJ0ZmF4r2uuo3tdMx/Y1MYnP7iJ7nVNxa9mutY20VwfJZd3snkn78XH/NXHnDvZ3I/vy819+dXl+X3u5HIL9y3VvvTrQ73rVvz9U7iLrDB3ZyqTY3zmauhOzi5av0ZIT8zkmCw+lq5PzuZuua6IwZpi0K9pvBr6iz8ISvfN72+MEbvFbgV35/JkZkFwD19aGOBTmYU/59rmOrrXNXFXZysf29FJz/rmq+G9ronWhuUjLRY1YtFbKr0qKNwlVNydTM7J5PLMZvNkcnlmio+ZnDObzTNbsm+u3dVtzmw2V3gsaTf/WHyOhdsWLmeyzkw2x8Rs4ch6MpPDvbz6oxGjpT5Ka0OM5oYYLfVRmutj3Lm2npaGwnJLfZSWhtjV9YYoLfUxWhpiNM/vK7QzM8amMoxNZ0hPZQrLU1nSU5n5r7l96akM59NTpKeyjE1lmM1dv0ujtSHGmsbY/AfEXPBf/VCI0d5cWJ7J5Bccec91nyz+kGpvKoR3f2cLH93ROR/cPeub6FrbRFtj3c3+16g5ZYW7mT0KfA2IAl93999btP8PgZ8prjYDG919bSULFZnj7lycmGVoZILTF8YZGpng7ZEJhi6Mc/biJNl8mUlapljEqI9FqItGqI9FqI9GqIsu3FYXjdDSEGPt/L4orSVh3FwSuM31sWJ4F0K5eT7Mo9RHIxXvKmlvuvFAdHdmsvmFHwILlrM/9uFw9tLkfJuJa/xl0dYQo3t9M1s3tPBTd3Uu7DZZ13RTtcrSlg13M4sCzwGPAEngiJkddPcTc23c/ddK2v8KcP8K1Co1ZjqT452LEwyNTDA0Ms7QhavLY9PZ+Xb1sQhbNzSzY2Mbjwxsoq0htiB0rwby3Dab3zbXpi4aoSH2423qIhEikdXbL71SzIzGuiiNdVE2rWm84e/P5PLFvxgKHwKxiNGzvlnhfRuVc+S+Gzjl7kMAZvY88Dhw4hrtnwB+pzLlSdjl8867Y9OF0C4ehRdCfJzU6NSC7ozN7Y30dbSw5yfupL+jlf7OFrZ1tnLn2iaiNRjAq1ldNMKG1gY2tDYEXUrNKifcu4DhkvUk8OBSDc1sC9AHfO/WS5MwuTKd4XTJkffbFyY4PTLB6QsTCwbNWuqj9He28kDvOj73oW76O1vp72ihr6OFljIGy0SkoNK/LXuBF9x9yQ43M9sH7APo7e2t8EtL0LK5wqDZUGk/eLE7ZeTKzHy7iEHP+mb6O1p4qH8D/Z0t80fhG9saVvXpeSLVopxwTwE9JevdxW1L2Qt88VpP5O77gf0Ag4ODlR31ktvu0sQsL5++yA/evsjh05d4e2ScTO7qP+u65jr6i6es9XW20N/RyrbOFno3NNNQC+eiiQSonHA/Amw3sz4Kob4XeHJxIzO7G1gH/KCiFcqqkZ7M8NLpi7w0VAj0H757BYDm+igf2rKOn7l7I30dLWwrBvm6lvqAKxapXcuGu7tnzewp4EUKp0J+w92Pm9mzwFF3P1hsuhd43r3cM3pltRubzvDy0KVCmA9d5MT5MdyhsS7C4Jb1/LNPbebhbRvY1b12xe+TISI3xoLK4sHBQdcE2avL+EyWI6evhvmxVJq8F041fKB3LQ/3d/Dwtg3c19OubhWRgJjZK+4+uFw7nX5QwyZnsxx55/J8N0silSaXd+qixv0963jq49t5uH8D9/eupbFOYS5STRTuNWRqNscrZy7zg6ELvDR0iTeGR8nmnVjEuK9nLf/kp7fx8LYNPNC7jqZ6hblINVO4h9h0JserZy/z0tsXeWnoEq8NXyaTc6IRY2dXO7/80X4e7t/A4NZ1NNfrv4JImOg3OkRmsjlePzvKD4YKZ7S8enaU2WyeiMG9Xe38w4/08dC2DQxuWacbMImEnMK9yh1Lpfn+D9/nB0MXeeXMZWayecxgYPMa/v5DW3iofwM/2bde9/QQqTEK9yo0PpPl4OvnOPDyGY6lxgC4+442nnywl4f6N/Bg33rWNuscc5FapnCvIsdSaQ68fJb/9lqKidkcH9jUxr/ecw+f2bVZN2gSkQUU7qvcxEyWP3/jHAdePks8maYhFuEzu+7kyQd7eKB3ne7DIiJLUrivUifOjXHg5TP82WvnGJ/Jsn1jK7/ztwf4ufu7aW9W/7mIXJ/CfRWZnM3yF2+c55svn+WN4VHqYxE+vXMzTz7Yy+AWHaWLSPkU7qvAD98d48Dhs/zXV1NcmcmyrbOFf/mZAT77QJcGRkXkpijcAzI1m+MvE+c5cPgMr54dpT4a4bGdd/Dk7l52963XUbqI3BKF+2321ntXOHD4LN99NcnYdJb+jhZ++9Mf5Oce6Ga9bpErIhWicL8NpjM5DiXOc+DwWY6euUxd1Hj03s08ubuXh/p1lC4iladwX0Gn3r/CNw+f5buvpkhPZejraOG3/tbdfPaBbp2XLiIrSuFeYdOZHH917F0OHD7Ly+9coi5qfOqeO/h7uwtXj0YiOkoXkZWncK+Qt0fG+dbhs3zn1SSXJzNs2dDM04/dzec+1E2HjtJF5DZTuN+CmWzhKP1bL5/lpaFLxCLGp+7ZxJO7t/DhbTpKF5HglBXuZvYo8DUKc6h+3d1/b4k2fwf4V4ADb7j7j02iHSZ/feI9fvM7cS5NzNKzvonf+Jsf4POD3Wxsawy6NBGR5cPdzKLAc8AjQBI4YmYH3f1ESZvtwFeAj7j7ZTPbuFIFrxb//v8M0Vwf5Y/+7m5+6q4OHaWLyKpSzpT1u4FT7j7k7rPA88Dji9r8MvCcu18GcPf3K1vm6pLLO8dTaT5x90Y+uqNTwS4iq0454d4FDJesJ4vbSu0AdpjZ/zOzl4rdOKE1NDLOxGyOnd1rgy5FRGRJlRpQjQHbgY8B3cD/NrOd7j5a2sjM9gH7AHp7eyv00rdfPJkGYFd3e8CViIgsrZwj9xTQU7LeXdxWKgkcdPeMu58G3qIQ9gu4+353H3T3wc7OzputOXCJVJrm+ijbOluDLkVEZEnlhPsRYLuZ9ZlZPbAXOLiozZ9ROGrHzDoodNMMVbDOVSWeHOXeO9uJqq9dRFapZcPd3bPAU8CLwEng2+5+3MyeNbM9xWYvAhfN7ATwfeA33P3iShUdpGwuz/FzY+xUl4yIrGJl9bm7+yHg0KJtz5QsO/Dl4leo/ej9cWayefW3i8iqVk63jJSIJwtjxDu7FO4isnop3G9QPJmmrSHG1g0tQZciInJNCvcblEil2dndrguXRGRVU7jfgJlsjpPnNZgqIqufwv0GvPXuOJmcs6tLV6aKyOqmcL8BbxQHU3WmjIisdgr3G5BIplnXXEf3uqagSxERuS6F+w2Ip9Ls7F6rCa1FZNVTuJdpOpPjrfeusEvnt4tIFVC4l+nE+TFyedeZMiJSFRTuZYoPazBVRKqHwr1M8VSajtYG7lijOVJFZPVTuJcpkUyzq7tdg6kiUhUU7mWYmMlyamRcXTIiUjUU7mU4fm4Md/W3i0j1ULiXYe42v/fqNEgRqRIK9zLEk2k2tzeysU2DqSJSHRTuZUik0pqcQ0SqSlnhbmaPmtmbZnbKzJ5eYv8XzGzEzF4vfv2jypcajPRUhtMXJrivR3eCFJHqsewcqmYWBZ4DHgGSwBEzO+juJxY1/VN3f2oFagzU8VQa0LR6IlJdyjly3w2ccvchd58FngceX9myVo+4wl1EqlA54d4FDJesJ4vbFvusmcXN7AUz66lIdatAPDlKz/om1rXUB12KiEjZKjWg+ufAVnffBfw18CdLNTKzfWZ21MyOjoyMVOilV1Y8mdbMSyJSdcoJ9xRQeiTeXdw2z90vuvtMcfXrwIeWeiJ33+/ug+4+2NnZeTP13laXJmZJXp7SxUsiUnXKCfcjwHYz6zOzemAvcLC0gZltLlndA5ysXInBScz1tyvcRaTKLHu2jLtnzewp4EUgCnzD3Y+b2bPAUXc/CPyqme0BssAl4AsrWPNtk9CVqSJSpZYNdwB3PwQcWrTtmZLlrwBfqWxpwYsn0/R3tLCmsS7oUkREboiuUL2OeDKtLhkRqUoK92t4f2yad8emdX67iFQlhfs1zA2m6rYDIlKNFO7XEE+miRgMbF4TdCkiIjdM4X4NiVSauza20tJQ1piziMiqonBfgrsTT46yU1emikiVUrgv4Xx6mgvjs7oyVUSqlsJ9CfFkYTBV4S4i1UrhvoREapRYxPigBlNFpEop3JcQT6bZsamNxrpo0KWIiNwUhfsi7k4ilVaXjIhUNYX7IsOXphidzOi2AyJS1RTui8RThTtB3tet0yBFpHop3BdJJNPURyPs2NQWdCkiIjdN4b5IPJnmg5vbqI/prRGR6qUEK5HPO8dSus2viFQ/hXuJ0xcnuDKT1YTYIlL1FO4lEknNmSoi4aBwLxFPpmmsi7B9Y2vQpYiI3JKywt3MHjWzN83slJk9fZ12nzUzN7PBypV4+yRSo9xzZzuxqD7zRKS6LZtiZhYFngMeAwaAJ8xsYIl2bcCXgMOVLvJ2yOWdY6kxTasnIqFQziHqbuCUuw+5+yzwPPD4Eu1+F/h9YLqC9d02b4+MM5XJ6bYDIhIK5YR7FzBcsp4sbptnZg8APe7+l9d7IjPbZ2ZHzezoyMjIDRe7kt4YLlyZqnAXkTC45c5lM4sAXwV+fbm27r7f3QfdfbCzs/NWX7qiEqk0LfVR+js0mCoi1a+ccE8BPSXr3cVtc9qAe4H/ZWbvAA8BB6ttUDWeTHNvVzuRiAVdiojILSsn3I8A282sz8zqgb3Awbmd7p529w533+ruW4GXgD3ufnRFKl4BmVyeE+fH1CUjIqGxbLi7exZ4CngROAl8292Pm9mzZrZnpQu8Hd567wqz2Tw7dSdIEQmJWDmN3P0QcGjRtmeu0fZjt17W7TU/Z6pOgxSRkNDVOhTCfU1jjC0bmoMuRUSkIhTuFK5M3dW9FjMNpopIONR8uE9ncrz57hXdLExEQqXmw/3Nd6+Qybn620UkVGo+3OMp3eZXRMJH4T48yvqWerrWNgVdiohIxdR8uCdSaXZ1t2swVURCpabDfWo2x1vvXVF/u4iETk2H+4nzafKOrkwVkdCp6XCfvzJVg6kiEjI1H+4b2xrYtKYx6FJERCqqxsO9cGWqiEjY1Gy4X5nOMHRhQl0yIhJKNRvux8+N4a6Ll0QknGo23BPFwdSdOg1SREKoZsM9nkrTtbaJjtaGoEsREam42g335KiO2kUktGoy3NOTGc5cnGRXj8JdRMKprHA3s0fN7E0zO2VmTy+x/x+bWcLMXjez/2tmA5UvtXISqblp9XQapIiE07LhbmZR4DngMWAAeGKJ8D7g7jvd/SeAPwC+WvFKKyieGgU0mCoi4VXOkftu4JS7D7n7LPA88HhpA3cfK1ltAbxyJVZeIplmy4Zm2pvrgi5FRGRFxMpo0wUMl6wngQcXNzKzLwJfBuqBjy/1RGa2D9gH0Nvbe6O1Vkw8meb+XnXJiEh4VWxA1d2fc/dtwG8Cv32NNvvdfdDdBzs7Oyv10jfkwvgMqdEp7tNtB0QkxMoJ9xTQU7LeXdx2Lc8DP3srRa2khKbVE5EaUE64HwG2m1mfmdUDe4GDpQ3MbHvJ6qeBH1WuxMpKJNOYwT13rgm6FBGRFbNsn7u7Z83sKeBFIAp8w92Pm9mzwFF3Pwg8ZWafBDLAZeAXV7LoWxFPpunvaKGtUYOpIhJe5Qyo4u6HgEOLtj1TsvylCte1YhKpUT68rSPoMkREVlRNXaH63tg0743N6Da/IhJ6NRXumlZPRGpFTYV7IjlKxGBgs8JdRMKtpsI9nkqzY1MbTfXRoEsREVlRNRPu7k4imdb9ZESkJtRMuKdGp7g4Mav+dhGpCTUT7on5wVTddkBEwq9mwj2eSlMXNe7e3BZ0KSIiK65mwj2RTPOBO9poiGkwVUTCrybC3d2Lc6aqS0ZEakNNhPuZi5OMTWc1mCoiNaMmwj2e0pWpIlJbaiLcE8lR6mMRdmzSYKqI1IaaCPd4Ms3A5jXURWvixxURCX+45/POsVRaXTIiUlNCH+5DFyaYmM3ptgMiUlNCH+7x5CgA9/XoNEgRqR01EO5pmuqibOtsDboUEZHbpqxwN7NHzexNMztlZk8vsf/LZnbCzOJm9j/NbEvlS705iVSae7vWEI1Y0KWIiNw2y4a7mUWB54DHgAHgCTMbWNTsNWDQ3XcBLwB/UOlCb0Y2l+f4ubSuTBWRmlPOkftu4JS7D7n7LPA88HhpA3f/vrtPFldfArorW+bNOTUyznQmrzNlRKTmlBPuXcBwyXqyuO1afgn477dSVKXEhwtXpu5UuItIjYlV8snM7OeBQeCnr7F/H7APoLe3t5IvvaR4apS2hhh9G1pW/LVERFaTco7cU0BPyXp3cdsCZvZJ4F8Ae9x9Zqkncvf97j7o7oOdnZ03U+8NSSTT3NvVTkSDqSJSY8oJ9yPAdjPrM7N6YC9wsLSBmd0P/DGFYH+/8mXeuNlsnpPnr6i/XURq0rLh7u5Z4CngReAk8G13P25mz5rZnmKzfwO0Av/FzF43s4PXeLrb5q33rjCby6u/XURqUll97u5+CDi0aNszJcufrHBdtyw+N2eqToMUkRoU2itU48lR1jbX0bO+KehSRERuuxCHe5qdXe2YaTBVRGpPKMN9OpPjrfc0mCoitSuU4X7y/BjZvOu2AyJSs0IZ7gnNmSoiNS6U4f7GcJqO1gY2tzcGXYqISCBCGe6J1Ci7ujWYKiK1K3ThPjGT5dT745pWT0RqWujC/cT5MfKu/nYRqW2hC/e5K1N15C4itSx04Z5IjnLHmkY2rtFgqojUrtCFezyZVpeMiNS8UIX72HSGoQsTCncRqXmhCvdjqblp9XRlqojUtlCFe0KDqSIiQMjCPZ5K072uifUt9UGXIiISqHCFe3KU+9QlIyISnnC/PDHL8KUpTasnIkKZ4W5mj5rZm2Z2ysyeXmL/R83sVTPLmtnnKl/m8ubvBKn+dhGR5cPdzKLAc8BjwADwhJkNLGp2FvgCcKDSBZZrLtzvUbiLiJQ1QfZu4JS7DwGY2fPA48CJuQbu/k5xX34FaixLPDlKX0cL7U11QZUgIrJqlNMt0wUMl6wni9tWlURxzlQREbnNA6pmts/MjprZ0ZGRkYo978iVGc6lp3VlqohIUTnhngJ6Sta7i9tumLvvd/dBdx/s7Oy8madYUiI1CsAunQYpIgKUF+5HgO1m1mdm9cBe4ODKlnVj4sk0ZnDPnWuCLkVEZFVYNtzdPQs8BbwInAS+7e7HzexZM9sDYGY/aWZJ4PPAH5vZ8ZUserFEMs1dna20NJQzPiwiEn5lpaG7HwIOLdr2TMnyEQrdNbeduxNPpfkb2zuCeHkRkVWp6q9QfXdsmpErM7rtgIhIiaoP9/lp9XSmjIjIvKoP90QyTTRiDGzWYKqIyJyqD/d4Ks2OTW001kWDLkVEZNWo6nB3dxLJUd0sTERkkaoO9+TlKS5PZtjVo3AXESlV1eE+N5i6q0tnyoiIlKrucE+NUh+NsOOO1qBLERFZVao63BPJNHdvbqMhpsFUEZFSVRvu+byTSOk2vyIiS6nacD9zaZIr01nd5ldEZAlVG+7xpG7zKyJyLVUc7mkaYhG2b9RgqojIYlUb7olkmnvuXEMsWrU/gojIiqnKZMzlnWPn0uqSERG5hqoM96GRcSZnczpTRkTkGqoy3OeuTL1Ptx0QEVlSlYb7KC31Ufo6NJgqIrKUssLdzB41szfN7JSZPb3E/gYz+9Pi/sNmtrXShZaKp9Lc09VONGIr+TIiIlVr2XA3syjwHPAYMAA8YWYDi5r9EnDZ3e8C/hD4/UoXOieTy3Pi3Jhu8ysich3lHLnvBk65+5C7zwLPA48vavM48CfF5ReAT5jZihxW/+i9cWayeU2rJyJyHeWEexcwXLKeLG5bso27Z4E0sKESBS6WSBWuTNWE2CIi13ZbB1TNbJ+ZHTWzoyMjIzf1HOtbGvjUwCa2bGiucHUiIuERK6NNCugpWe8ubluqTdLMYkA7cHHxE7n7fmA/wODgoN9MwY8MbOKRgU03860iIjWjnCP3I8B2M+szs3pgL3BwUZuDwC8Wlz8HfM/dbyq8RUTk1i175O7uWTN7CngRiALfcPfjZvYscNTdDwL/AfhPZnYKuEThA0BERAJSTrcM7n4IOLRo2zMly9PA5ytbmoiI3KyqvEJVRESuT+EuIhJCCncRkRBSuIuIhJDCXUQkhCyo09HNbAQ4c5Pf3gFcqGA51U7vx0J6P67Se7FQGN6PLe7euVyjwML9VpjZUXcfDLqO1ULvx0J6P67Se7FQLb0f6pYREQkhhbuISAhVa7jvD7qAVUbvx0J6P67Se7FQzbwfVdnnLiIi11etR+4iInIdVRfuy03WXSvMrMfMvm9mJ8zsuJl9KeiaVgMzi5rZa2b2F0HXEjQzW2tmL5jZD83spJk9HHRNQTGzXyv+nhwzs2+ZWWPQNa20qgr3MifrrhVZ4NfdfQB4CPhiDb8Xpb4EnAy6iFXia8BfufvdwH3U6PtiZl3ArwKD7n4vhVuXh/625FUV7pQ3WXdNcPfz7v5qcfkKhV/cxXPb1hQz6wY+DXw96FqCZmbtwEcpzLWAu8+6+2iwVQUqBjQVZ4prBs4FXM+Kq7ZwL2ey7ppjZluB+4HDwVYSuD8C/jmQD7qQVaAPGAH+Y7Gb6utm1hJ0UUFw9xTwb4GzwHkg7e7/I9iqVl61hbssYmatwHeAf+ruY0HXExQz+wzwvru/EnQtq0QMeAD4d+5+PzAB1OQYlZmto/AXfh9wJ9BiZj8fbFUrr9rCvZzJumuGmdVRCPZvuvt3g64nYB8B9pjZOxS66z5uZv852JIClQSS7j7319wLFMK+Fn0SOO3uI+6eAb4LfDjgmlZctYV7OZN11wQzMwr9qSfd/atB1xM0d/+Ku3e7+1YK/y++5+6hPzq7Fnd/Fxg2sw8UN30COBFgSUE6CzxkZs3F35tPUAODy2XNobpaXGuy7oDLCspHgF8AEmb2enHbbxXnuxUB+BXgm8UDoSHgHwRcTyDc/bCZvQC8SuEss9eogStVdYWqiEgIVVu3jIiIlEHhLiISQgp3EZEQUriLiISQwl1EJIQU7iIiIaRwFxEJIYW7iEgI/X+D9z1EmoEnOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49c46ab5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot_history(evaluation_function.fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By including the context, our f-score increase by over 15 points!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to try yourself\n",
    "\n",
    "Try altering the structure of the network. You could, for example\n",
    "\n",
    "- Try different RNNs (LSTM, biLSTM)\n",
    "- Try using CNNs (hint: use the Conv1D layer, and the Dense layer afterwards needs a TimeDistributed wrapper layer)\n",
    "\n",
    "Again, the layers supported by Keras can be found from its documentation: https://keras.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KnpFCI-IR3E"
   },
   "source": [
    "### The simple model, written in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yZ2pE4ekB4I0",
    "outputId": "6aa10a8c-d2c0-4191-9fd1-efeb81c95ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "x_train shape: torch.Size([25000, 400])\n",
      "x_test shape: torch.Size([25000, 400])\n",
      "tensor([[  0,   0,   0,  ...,  19, 178,  32],\n",
      "        [  0,   0,   0,  ...,  16, 145,  95],\n",
      "        [  0,   0,   0,  ...,   7, 129, 113],\n",
      "        ...,\n",
      "        [595,  13, 258,  ...,  72,  33,  32],\n",
      "        [  0,   0,   0,  ...,  28, 126, 110],\n",
      "        [  0,   0,   0,  ...,   7,  43,  50]])\n",
      "[1 0 0 1 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   161] loss: 0.409\n",
      "[1,   321] loss: 0.352\n",
      "[1,   481] loss: 0.345\n",
      "[1,   641] loss: 0.346\n",
      "[1,   801] loss: 0.348\n",
      "[1,   961] loss: 0.339\n",
      "[1,  1121] loss: 0.346\n",
      "[1,  1281] loss: 0.345\n",
      "[1,  1441] loss: 0.350\n",
      "[1,  1601] loss: 0.336\n",
      "[1,  1761] loss: 0.350\n",
      "[1,  1921] loss: 0.348\n",
      "[1,  2081] loss: 0.341\n",
      "[1,  2241] loss: 0.336\n",
      "[1,  2401] loss: 0.343\n",
      "[1,  2561] loss: 0.340\n",
      "[1,  2721] loss: 0.339\n",
      "[1,  2881] loss: 0.340\n",
      "[1,  3041] loss: 0.335\n",
      "[1,  3201] loss: 0.336\n",
      "[1,  3361] loss: 0.336\n",
      "[1,  3521] loss: 0.331\n",
      "[1,  3681] loss: 0.329\n",
      "[1,  3841] loss: 0.331\n",
      "[1,  4001] loss: 0.333\n",
      "[1,  4161] loss: 0.326\n",
      "[1,  4321] loss: 0.331\n",
      "[1,  4481] loss: 0.326\n",
      "[1,  4641] loss: 0.328\n",
      "[1,  4801] loss: 0.333\n",
      "[1,  4961] loss: 0.333\n",
      "[1,  5121] loss: 0.323\n",
      "[1,  5281] loss: 0.317\n",
      "[1,  5441] loss: 0.323\n",
      "[1,  5601] loss: 0.315\n",
      "[1,  5761] loss: 0.322\n",
      "[1,  5921] loss: 0.319\n",
      "[1,  6081] loss: 0.323\n",
      "[1,  6241] loss: 0.324\n",
      "[1,  6401] loss: 0.311\n",
      "[1,  6561] loss: 0.316\n",
      "[1,  6721] loss: 0.307\n",
      "[1,  6881] loss: 0.309\n",
      "[1,  7041] loss: 0.306\n",
      "[1,  7201] loss: 0.316\n",
      "[1,  7361] loss: 0.305\n",
      "[1,  7521] loss: 0.301\n",
      "[1,  7681] loss: 0.323\n",
      "[1,  7841] loss: 0.302\n",
      "[1,  8001] loss: 0.307\n",
      "[1,  8161] loss: 0.292\n",
      "[1,  8321] loss: 0.295\n",
      "[1,  8481] loss: 0.313\n",
      "[1,  8641] loss: 0.300\n",
      "[1,  8801] loss: 0.308\n",
      "[1,  8961] loss: 0.288\n",
      "[1,  9121] loss: 0.292\n",
      "[1,  9281] loss: 0.300\n",
      "[1,  9441] loss: 0.299\n",
      "[1,  9601] loss: 0.298\n",
      "[1,  9761] loss: 0.280\n",
      "[1,  9921] loss: 0.300\n",
      "[1, 10081] loss: 0.302\n",
      "[1, 10241] loss: 0.273\n",
      "[1, 10401] loss: 0.292\n",
      "[1, 10561] loss: 0.304\n",
      "[1, 10721] loss: 0.283\n",
      "[1, 10881] loss: 0.304\n",
      "[1, 11041] loss: 0.310\n",
      "[1, 11201] loss: 0.293\n",
      "[1, 11361] loss: 0.296\n",
      "[1, 11521] loss: 0.287\n",
      "[1, 11681] loss: 0.296\n",
      "[1, 11841] loss: 0.296\n",
      "[1, 12001] loss: 0.275\n",
      "[1, 12161] loss: 0.276\n",
      "[1, 12321] loss: 0.288\n",
      "[1, 12481] loss: 0.298\n",
      "[1, 12641] loss: 0.288\n",
      "[1, 12801] loss: 0.288\n",
      "[1, 12961] loss: 0.279\n",
      "[1, 13121] loss: 0.275\n",
      "[1, 13281] loss: 0.271\n",
      "[1, 13441] loss: 0.280\n",
      "[1, 13601] loss: 0.277\n",
      "[1, 13761] loss: 0.295\n",
      "[1, 13921] loss: 0.284\n",
      "[1, 14081] loss: 0.296\n",
      "[1, 14241] loss: 0.291\n",
      "[1, 14401] loss: 0.277\n",
      "[1, 14561] loss: 0.290\n",
      "[1, 14721] loss: 0.280\n",
      "[1, 14881] loss: 0.263\n",
      "[1, 15041] loss: 0.299\n",
      "[1, 15201] loss: 0.271\n",
      "[1, 15361] loss: 0.275\n",
      "[1, 15521] loss: 0.278\n",
      "[1, 15681] loss: 0.270\n",
      "[1, 15841] loss: 0.292\n",
      "[1, 16001] loss: 0.263\n",
      "[1, 16161] loss: 0.270\n",
      "[1, 16321] loss: 0.275\n",
      "[1, 16481] loss: 0.284\n",
      "[1, 16641] loss: 0.286\n",
      "[1, 16801] loss: 0.264\n",
      "[1, 16961] loss: 0.266\n",
      "[1, 17121] loss: 0.272\n",
      "[1, 17281] loss: 0.269\n",
      "[1, 17441] loss: 0.279\n",
      "[1, 17601] loss: 0.270\n",
      "[1, 17761] loss: 0.269\n",
      "[1, 17921] loss: 0.269\n",
      "[1, 18081] loss: 0.272\n",
      "[1, 18241] loss: 0.266\n",
      "[1, 18401] loss: 0.275\n",
      "[1, 18561] loss: 0.255\n",
      "[1, 18721] loss: 0.277\n",
      "[1, 18881] loss: 0.261\n",
      "[1, 19041] loss: 0.274\n",
      "[1, 19201] loss: 0.283\n",
      "[1, 19361] loss: 0.271\n",
      "[1, 19521] loss: 0.276\n",
      "[1, 19681] loss: 0.258\n",
      "[1, 19841] loss: 0.269\n",
      "[1, 20001] loss: 0.249\n",
      "[1, 20161] loss: 0.266\n",
      "[1, 20321] loss: 0.258\n",
      "[1, 20481] loss: 0.263\n",
      "[1, 20641] loss: 0.253\n",
      "[1, 20801] loss: 0.278\n",
      "[1, 20961] loss: 0.254\n",
      "[1, 21121] loss: 0.271\n",
      "[1, 21281] loss: 0.240\n",
      "[1, 21441] loss: 0.259\n",
      "[1, 21601] loss: 0.254\n",
      "[1, 21761] loss: 0.258\n",
      "[1, 21921] loss: 0.258\n",
      "[1, 22081] loss: 0.283\n",
      "[1, 22241] loss: 0.262\n",
      "[1, 22401] loss: 0.266\n",
      "[1, 22561] loss: 0.262\n",
      "[1, 22721] loss: 0.277\n",
      "[1, 22881] loss: 0.258\n",
      "[1, 23041] loss: 0.258\n",
      "[1, 23201] loss: 0.253\n",
      "[1, 23361] loss: 0.261\n",
      "[1, 23521] loss: 0.247\n",
      "[1, 23681] loss: 0.279\n",
      "[1, 23841] loss: 0.246\n",
      "[1, 24001] loss: 0.245\n",
      "[1, 24161] loss: 0.254\n",
      "[1, 24321] loss: 0.255\n",
      "[1, 24481] loss: 0.263\n",
      "[1, 24641] loss: 0.279\n",
      "[1, 24801] loss: 0.260\n",
      "[1, 24961] loss: 0.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.79588\n",
      "[2,   161] loss: 0.288\n",
      "[2,   321] loss: 0.256\n",
      "[2,   481] loss: 0.254\n",
      "[2,   641] loss: 0.262\n",
      "[2,   801] loss: 0.248\n",
      "[2,   961] loss: 0.264\n",
      "[2,  1121] loss: 0.248\n",
      "[2,  1281] loss: 0.262\n",
      "[2,  1441] loss: 0.242\n",
      "[2,  1601] loss: 0.245\n",
      "[2,  1761] loss: 0.263\n",
      "[2,  1921] loss: 0.248\n",
      "[2,  2081] loss: 0.264\n",
      "[2,  2241] loss: 0.265\n",
      "[2,  2401] loss: 0.266\n",
      "[2,  2561] loss: 0.262\n",
      "[2,  2721] loss: 0.266\n",
      "[2,  2881] loss: 0.259\n",
      "[2,  3041] loss: 0.255\n",
      "[2,  3201] loss: 0.240\n",
      "[2,  3361] loss: 0.257\n",
      "[2,  3521] loss: 0.237\n",
      "[2,  3681] loss: 0.238\n",
      "[2,  3841] loss: 0.252\n",
      "[2,  4001] loss: 0.260\n",
      "[2,  4161] loss: 0.247\n",
      "[2,  4321] loss: 0.244\n",
      "[2,  4481] loss: 0.244\n",
      "[2,  4641] loss: 0.262\n",
      "[2,  4801] loss: 0.250\n",
      "[2,  4961] loss: 0.270\n",
      "[2,  5121] loss: 0.241\n",
      "[2,  5281] loss: 0.230\n",
      "[2,  5441] loss: 0.241\n",
      "[2,  5601] loss: 0.249\n",
      "[2,  5761] loss: 0.249\n",
      "[2,  5921] loss: 0.245\n",
      "[2,  6081] loss: 0.253\n",
      "[2,  6241] loss: 0.265\n",
      "[2,  6401] loss: 0.233\n",
      "[2,  6561] loss: 0.237\n",
      "[2,  6721] loss: 0.243\n",
      "[2,  6881] loss: 0.238\n",
      "[2,  7041] loss: 0.246\n",
      "[2,  7201] loss: 0.252\n",
      "[2,  7361] loss: 0.241\n",
      "[2,  7521] loss: 0.230\n",
      "[2,  7681] loss: 0.260\n",
      "[2,  7841] loss: 0.236\n",
      "[2,  8001] loss: 0.227\n",
      "[2,  8161] loss: 0.229\n",
      "[2,  8321] loss: 0.237\n",
      "[2,  8481] loss: 0.251\n",
      "[2,  8641] loss: 0.236\n",
      "[2,  8801] loss: 0.262\n",
      "[2,  8961] loss: 0.228\n",
      "[2,  9121] loss: 0.234\n",
      "[2,  9281] loss: 0.245\n",
      "[2,  9441] loss: 0.246\n",
      "[2,  9601] loss: 0.238\n",
      "[2,  9761] loss: 0.216\n",
      "[2,  9921] loss: 0.248\n",
      "[2, 10081] loss: 0.246\n",
      "[2, 10241] loss: 0.226\n",
      "[2, 10401] loss: 0.250\n",
      "[2, 10561] loss: 0.243\n",
      "[2, 10721] loss: 0.230\n",
      "[2, 10881] loss: 0.246\n",
      "[2, 11041] loss: 0.252\n",
      "[2, 11201] loss: 0.243\n",
      "[2, 11361] loss: 0.248\n",
      "[2, 11521] loss: 0.239\n",
      "[2, 11681] loss: 0.260\n",
      "[2, 11841] loss: 0.247\n",
      "[2, 12001] loss: 0.230\n",
      "[2, 12161] loss: 0.229\n",
      "[2, 12321] loss: 0.259\n",
      "[2, 12481] loss: 0.266\n",
      "[2, 12641] loss: 0.243\n",
      "[2, 12801] loss: 0.248\n",
      "[2, 12961] loss: 0.237\n",
      "[2, 13121] loss: 0.228\n",
      "[2, 13281] loss: 0.233\n",
      "[2, 13441] loss: 0.243\n",
      "[2, 13601] loss: 0.229\n",
      "[2, 13761] loss: 0.254\n",
      "[2, 13921] loss: 0.244\n",
      "[2, 14081] loss: 0.254\n",
      "[2, 14241] loss: 0.254\n",
      "[2, 14401] loss: 0.230\n",
      "[2, 14561] loss: 0.242\n",
      "[2, 14721] loss: 0.250\n",
      "[2, 14881] loss: 0.231\n",
      "[2, 15041] loss: 0.259\n",
      "[2, 15201] loss: 0.227\n",
      "[2, 15361] loss: 0.238\n",
      "[2, 15521] loss: 0.239\n",
      "[2, 15681] loss: 0.227\n",
      "[2, 15841] loss: 0.249\n",
      "[2, 16001] loss: 0.224\n",
      "[2, 16161] loss: 0.233\n",
      "[2, 16321] loss: 0.249\n",
      "[2, 16481] loss: 0.250\n",
      "[2, 16641] loss: 0.250\n",
      "[2, 16801] loss: 0.230\n",
      "[2, 16961] loss: 0.230\n",
      "[2, 17121] loss: 0.239\n",
      "[2, 17281] loss: 0.238\n",
      "[2, 17441] loss: 0.245\n",
      "[2, 17601] loss: 0.231\n",
      "[2, 17761] loss: 0.238\n",
      "[2, 17921] loss: 0.238\n",
      "[2, 18081] loss: 0.229\n",
      "[2, 18241] loss: 0.230\n",
      "[2, 18401] loss: 0.243\n",
      "[2, 18561] loss: 0.228\n",
      "[2, 18721] loss: 0.234\n",
      "[2, 18881] loss: 0.238\n",
      "[2, 19041] loss: 0.242\n",
      "[2, 19201] loss: 0.240\n",
      "[2, 19361] loss: 0.231\n",
      "[2, 19521] loss: 0.244\n",
      "[2, 19681] loss: 0.219\n",
      "[2, 19841] loss: 0.233\n",
      "[2, 20001] loss: 0.226\n",
      "[2, 20161] loss: 0.233\n",
      "[2, 20321] loss: 0.230\n",
      "[2, 20481] loss: 0.240\n",
      "[2, 20641] loss: 0.237\n",
      "[2, 20801] loss: 0.246\n",
      "[2, 20961] loss: 0.226\n",
      "[2, 21121] loss: 0.240\n",
      "[2, 21281] loss: 0.215\n",
      "[2, 21441] loss: 0.230\n",
      "[2, 21601] loss: 0.225\n",
      "[2, 21761] loss: 0.229\n",
      "[2, 21921] loss: 0.219\n",
      "[2, 22081] loss: 0.253\n",
      "[2, 22241] loss: 0.238\n",
      "[2, 22401] loss: 0.239\n",
      "[2, 22561] loss: 0.243\n",
      "[2, 22721] loss: 0.249\n",
      "[2, 22881] loss: 0.238\n",
      "[2, 23041] loss: 0.232\n",
      "[2, 23201] loss: 0.225\n",
      "[2, 23361] loss: 0.233\n",
      "[2, 23521] loss: 0.217\n",
      "[2, 23681] loss: 0.251\n",
      "[2, 23841] loss: 0.223\n",
      "[2, 24001] loss: 0.214\n",
      "[2, 24161] loss: 0.233\n",
      "[2, 24321] loss: 0.233\n",
      "[2, 24481] loss: 0.244\n",
      "[2, 24641] loss: 0.246\n",
      "[2, 24801] loss: 0.229\n",
      "[2, 24961] loss: 0.230\n",
      "acc 0.8312\n",
      "[3,   161] loss: 0.265\n",
      "[3,   321] loss: 0.230\n",
      "[3,   481] loss: 0.221\n",
      "[3,   641] loss: 0.234\n",
      "[3,   801] loss: 0.221\n",
      "[3,   961] loss: 0.241\n",
      "[3,  1121] loss: 0.225\n",
      "[3,  1281] loss: 0.231\n",
      "[3,  1441] loss: 0.220\n",
      "[3,  1601] loss: 0.224\n",
      "[3,  1761] loss: 0.233\n",
      "[3,  1921] loss: 0.226\n",
      "[3,  2081] loss: 0.243\n",
      "[3,  2241] loss: 0.233\n",
      "[3,  2401] loss: 0.238\n",
      "[3,  2561] loss: 0.230\n",
      "[3,  2721] loss: 0.240\n",
      "[3,  2881] loss: 0.233\n",
      "[3,  3041] loss: 0.236\n",
      "[3,  3201] loss: 0.219\n",
      "[3,  3361] loss: 0.229\n",
      "[3,  3521] loss: 0.221\n",
      "[3,  3681] loss: 0.211\n",
      "[3,  3841] loss: 0.228\n",
      "[3,  4001] loss: 0.239\n",
      "[3,  4161] loss: 0.218\n",
      "[3,  4321] loss: 0.220\n",
      "[3,  4481] loss: 0.215\n",
      "[3,  4641] loss: 0.230\n",
      "[3,  4801] loss: 0.227\n",
      "[3,  4961] loss: 0.247\n",
      "[3,  5121] loss: 0.218\n",
      "[3,  5281] loss: 0.210\n",
      "[3,  5441] loss: 0.218\n",
      "[3,  5601] loss: 0.230\n",
      "[3,  5761] loss: 0.223\n",
      "[3,  5921] loss: 0.223\n",
      "[3,  6081] loss: 0.234\n",
      "[3,  6241] loss: 0.233\n",
      "[3,  6401] loss: 0.214\n",
      "[3,  6561] loss: 0.207\n",
      "[3,  6721] loss: 0.219\n",
      "[3,  6881] loss: 0.222\n",
      "[3,  7041] loss: 0.231\n",
      "[3,  7201] loss: 0.236\n",
      "[3,  7361] loss: 0.221\n",
      "[3,  7521] loss: 0.206\n",
      "[3,  7681] loss: 0.236\n",
      "[3,  7841] loss: 0.216\n",
      "[3,  8001] loss: 0.205\n",
      "[3,  8161] loss: 0.209\n",
      "[3,  8321] loss: 0.221\n",
      "[3,  8481] loss: 0.228\n",
      "[3,  8641] loss: 0.211\n",
      "[3,  8801] loss: 0.247\n",
      "[3,  8961] loss: 0.205\n",
      "[3,  9121] loss: 0.215\n",
      "[3,  9281] loss: 0.224\n",
      "[3,  9441] loss: 0.230\n",
      "[3,  9601] loss: 0.220\n",
      "[3,  9761] loss: 0.199\n",
      "[3,  9921] loss: 0.232\n",
      "[3, 10081] loss: 0.224\n",
      "[3, 10241] loss: 0.212\n",
      "[3, 10401] loss: 0.230\n",
      "[3, 10561] loss: 0.220\n",
      "[3, 10721] loss: 0.210\n",
      "[3, 10881] loss: 0.217\n",
      "[3, 11041] loss: 0.222\n",
      "[3, 11201] loss: 0.219\n",
      "[3, 11361] loss: 0.229\n",
      "[3, 11521] loss: 0.221\n",
      "[3, 11681] loss: 0.240\n",
      "[3, 11841] loss: 0.224\n",
      "[3, 12001] loss: 0.214\n",
      "[3, 12161] loss: 0.207\n",
      "[3, 12321] loss: 0.233\n",
      "[3, 12481] loss: 0.243\n",
      "[3, 12641] loss: 0.227\n",
      "[3, 12801] loss: 0.232\n",
      "[3, 12961] loss: 0.214\n",
      "[3, 13121] loss: 0.212\n",
      "[3, 13281] loss: 0.219\n",
      "[3, 13441] loss: 0.222\n",
      "[3, 13601] loss: 0.208\n",
      "[3, 13761] loss: 0.234\n",
      "[3, 13921] loss: 0.228\n",
      "[3, 14081] loss: 0.233\n",
      "[3, 14241] loss: 0.232\n",
      "[3, 14401] loss: 0.212\n",
      "[3, 14561] loss: 0.221\n",
      "[3, 14721] loss: 0.235\n",
      "[3, 14881] loss: 0.213\n",
      "[3, 15041] loss: 0.239\n",
      "[3, 15201] loss: 0.212\n",
      "[3, 15361] loss: 0.220\n",
      "[3, 15521] loss: 0.215\n",
      "[3, 15681] loss: 0.203\n",
      "[3, 15841] loss: 0.232\n",
      "[3, 16001] loss: 0.210\n",
      "[3, 16161] loss: 0.219\n",
      "[3, 16321] loss: 0.235\n",
      "[3, 16481] loss: 0.234\n",
      "[3, 16641] loss: 0.232\n",
      "[3, 16801] loss: 0.213\n",
      "[3, 16961] loss: 0.214\n",
      "[3, 17121] loss: 0.220\n",
      "[3, 17281] loss: 0.222\n",
      "[3, 17441] loss: 0.226\n",
      "[3, 17601] loss: 0.209\n",
      "[3, 17761] loss: 0.220\n",
      "[3, 17921] loss: 0.222\n",
      "[3, 18081] loss: 0.211\n",
      "[3, 18241] loss: 0.214\n",
      "[3, 18401] loss: 0.225\n",
      "[3, 18561] loss: 0.212\n",
      "[3, 18721] loss: 0.217\n",
      "[3, 18881] loss: 0.225\n",
      "[3, 19041] loss: 0.224\n",
      "[3, 19201] loss: 0.215\n",
      "[3, 19361] loss: 0.208\n",
      "[3, 19521] loss: 0.224\n",
      "[3, 19681] loss: 0.202\n",
      "[3, 19841] loss: 0.212\n",
      "[3, 20001] loss: 0.212\n",
      "[3, 20161] loss: 0.211\n",
      "[3, 20321] loss: 0.212\n",
      "[3, 20481] loss: 0.219\n",
      "[3, 20641] loss: 0.226\n",
      "[3, 20801] loss: 0.224\n",
      "[3, 20961] loss: 0.208\n",
      "[3, 21121] loss: 0.221\n",
      "[3, 21281] loss: 0.202\n",
      "[3, 21441] loss: 0.216\n",
      "[3, 21601] loss: 0.205\n",
      "[3, 21761] loss: 0.217\n",
      "[3, 21921] loss: 0.201\n",
      "[3, 22081] loss: 0.233\n",
      "[3, 22241] loss: 0.225\n",
      "[3, 22401] loss: 0.220\n",
      "[3, 22561] loss: 0.227\n",
      "[3, 22721] loss: 0.233\n",
      "[3, 22881] loss: 0.219\n",
      "[3, 23041] loss: 0.216\n",
      "[3, 23201] loss: 0.206\n",
      "[3, 23361] loss: 0.214\n",
      "[3, 23521] loss: 0.200\n",
      "[3, 23681] loss: 0.233\n",
      "[3, 23841] loss: 0.211\n",
      "[3, 24001] loss: 0.200\n",
      "[3, 24161] loss: 0.214\n",
      "[3, 24321] loss: 0.213\n",
      "[3, 24481] loss: 0.226\n",
      "[3, 24641] loss: 0.226\n",
      "[3, 24801] loss: 0.207\n",
      "[3, 24961] loss: 0.216\n",
      "acc 0.84648\n",
      "[4,   161] loss: 0.248\n",
      "[4,   321] loss: 0.219\n",
      "[4,   481] loss: 0.204\n",
      "[4,   641] loss: 0.214\n",
      "[4,   801] loss: 0.204\n",
      "[4,   961] loss: 0.221\n",
      "[4,  1121] loss: 0.212\n",
      "[4,  1281] loss: 0.217\n",
      "[4,  1441] loss: 0.209\n",
      "[4,  1601] loss: 0.207\n",
      "[4,  1761] loss: 0.217\n",
      "[4,  1921] loss: 0.212\n",
      "[4,  2081] loss: 0.224\n",
      "[4,  2241] loss: 0.215\n",
      "[4,  2401] loss: 0.222\n",
      "[4,  2561] loss: 0.217\n",
      "[4,  2721] loss: 0.222\n",
      "[4,  2881] loss: 0.220\n",
      "[4,  3041] loss: 0.224\n",
      "[4,  3201] loss: 0.202\n",
      "[4,  3361] loss: 0.217\n",
      "[4,  3521] loss: 0.207\n",
      "[4,  3681] loss: 0.196\n",
      "[4,  3841] loss: 0.210\n",
      "[4,  4001] loss: 0.219\n",
      "[4,  4161] loss: 0.200\n",
      "[4,  4321] loss: 0.206\n",
      "[4,  4481] loss: 0.199\n",
      "[4,  4641] loss: 0.212\n",
      "[4,  4801] loss: 0.215\n",
      "[4,  4961] loss: 0.234\n",
      "[4,  5121] loss: 0.204\n",
      "[4,  5281] loss: 0.197\n",
      "[4,  5441] loss: 0.209\n",
      "[4,  5601] loss: 0.222\n",
      "[4,  5761] loss: 0.208\n",
      "[4,  5921] loss: 0.210\n",
      "[4,  6081] loss: 0.218\n",
      "[4,  6241] loss: 0.214\n",
      "[4,  6401] loss: 0.204\n",
      "[4,  6561] loss: 0.192\n",
      "[4,  6721] loss: 0.204\n",
      "[4,  6881] loss: 0.209\n",
      "[4,  7041] loss: 0.214\n",
      "[4,  7201] loss: 0.218\n",
      "[4,  7361] loss: 0.212\n",
      "[4,  7521] loss: 0.191\n",
      "[4,  7681] loss: 0.218\n",
      "[4,  7841] loss: 0.203\n",
      "[4,  8001] loss: 0.192\n",
      "[4,  8161] loss: 0.195\n",
      "[4,  8321] loss: 0.206\n",
      "[4,  8481] loss: 0.215\n",
      "[4,  8641] loss: 0.189\n",
      "[4,  8801] loss: 0.230\n",
      "[4,  8961] loss: 0.190\n",
      "[4,  9121] loss: 0.202\n",
      "[4,  9281] loss: 0.212\n",
      "[4,  9441] loss: 0.220\n",
      "[4,  9601] loss: 0.205\n",
      "[4,  9761] loss: 0.189\n",
      "[4,  9921] loss: 0.218\n",
      "[4, 10081] loss: 0.210\n",
      "[4, 10241] loss: 0.205\n",
      "[4, 10401] loss: 0.215\n",
      "[4, 10561] loss: 0.208\n",
      "[4, 10721] loss: 0.197\n",
      "[4, 10881] loss: 0.203\n",
      "[4, 11041] loss: 0.208\n",
      "[4, 11201] loss: 0.206\n",
      "[4, 11361] loss: 0.212\n",
      "[4, 11521] loss: 0.210\n",
      "[4, 11681] loss: 0.228\n",
      "[4, 11841] loss: 0.206\n",
      "[4, 12001] loss: 0.202\n",
      "[4, 12161] loss: 0.193\n",
      "[4, 12321] loss: 0.213\n",
      "[4, 12481] loss: 0.228\n",
      "[4, 12641] loss: 0.212\n",
      "[4, 12801] loss: 0.220\n",
      "[4, 12961] loss: 0.201\n",
      "[4, 13121] loss: 0.198\n",
      "[4, 13281] loss: 0.205\n",
      "[4, 13441] loss: 0.210\n",
      "[4, 13601] loss: 0.195\n",
      "[4, 13761] loss: 0.213\n",
      "[4, 13921] loss: 0.217\n",
      "[4, 14081] loss: 0.221\n",
      "[4, 14241] loss: 0.217\n",
      "[4, 14401] loss: 0.198\n",
      "[4, 14561] loss: 0.207\n",
      "[4, 14721] loss: 0.222\n",
      "[4, 14881] loss: 0.201\n",
      "[4, 15041] loss: 0.219\n",
      "[4, 15201] loss: 0.201\n",
      "[4, 15361] loss: 0.208\n",
      "[4, 15521] loss: 0.199\n",
      "[4, 15681] loss: 0.194\n",
      "[4, 15841] loss: 0.221\n",
      "[4, 16001] loss: 0.195\n",
      "[4, 16161] loss: 0.206\n",
      "[4, 16321] loss: 0.223\n",
      "[4, 16481] loss: 0.217\n",
      "[4, 16641] loss: 0.216\n",
      "[4, 16801] loss: 0.202\n",
      "[4, 16961] loss: 0.200\n",
      "[4, 17121] loss: 0.209\n",
      "[4, 17281] loss: 0.210\n",
      "[4, 17441] loss: 0.214\n",
      "[4, 17601] loss: 0.191\n",
      "[4, 17761] loss: 0.205\n",
      "[4, 17921] loss: 0.210\n",
      "[4, 18081] loss: 0.200\n",
      "[4, 18241] loss: 0.206\n",
      "[4, 18401] loss: 0.209\n",
      "[4, 18561] loss: 0.201\n",
      "[4, 18721] loss: 0.207\n",
      "[4, 18881] loss: 0.215\n",
      "[4, 19041] loss: 0.208\n",
      "[4, 19201] loss: 0.201\n",
      "[4, 19361] loss: 0.195\n",
      "[4, 19521] loss: 0.209\n",
      "[4, 19681] loss: 0.187\n",
      "[4, 19841] loss: 0.203\n",
      "[4, 20001] loss: 0.203\n",
      "[4, 20161] loss: 0.199\n",
      "[4, 20321] loss: 0.199\n",
      "[4, 20481] loss: 0.206\n",
      "[4, 20641] loss: 0.212\n",
      "[4, 20801] loss: 0.209\n",
      "[4, 20961] loss: 0.194\n",
      "[4, 21121] loss: 0.213\n",
      "[4, 21281] loss: 0.192\n",
      "[4, 21441] loss: 0.203\n",
      "[4, 21601] loss: 0.193\n",
      "[4, 21761] loss: 0.208\n",
      "[4, 21921] loss: 0.186\n",
      "[4, 22081] loss: 0.215\n",
      "[4, 22241] loss: 0.209\n",
      "[4, 22401] loss: 0.205\n",
      "[4, 22561] loss: 0.214\n",
      "[4, 22721] loss: 0.212\n",
      "[4, 22881] loss: 0.203\n",
      "[4, 23041] loss: 0.204\n",
      "[4, 23201] loss: 0.195\n",
      "[4, 23361] loss: 0.200\n",
      "[4, 23521] loss: 0.188\n",
      "[4, 23681] loss: 0.217\n",
      "[4, 23841] loss: 0.203\n",
      "[4, 24001] loss: 0.190\n",
      "[4, 24161] loss: 0.202\n",
      "[4, 24321] loss: 0.200\n",
      "[4, 24481] loss: 0.211\n",
      "[4, 24641] loss: 0.211\n",
      "[4, 24801] loss: 0.194\n",
      "[4, 24961] loss: 0.203\n",
      "acc 0.85332\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 50\n",
    "kernel_size = 3\n",
    "epochs = 4\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "\n",
    "x_train = torch.LongTensor(sequence.pad_sequences(x_train, maxlen=maxlen))\n",
    "x_test = torch.LongTensor(sequence.pad_sequences(x_test, maxlen=maxlen))\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print (x_train[:10])\n",
    "print (y_train[:10])\n",
    "\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dims, max_features, filters, kernel_size, maxlen):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(max_features, embedding_dims)\n",
    "        self.conv1 = nn.Conv1d(embedding_dims, filters, kernel_size, padding=1)\n",
    "        self.max_pooling = nn.MaxPool1d(maxlen)\n",
    "        self.output_layer = nn.Linear(filters, 2)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #shape of input: [batch_size, maxlen]\n",
    "        x = self.embeddings(x)\n",
    "        #shape of x: [batch_size, maxlen, embedding_dim]\n",
    "        x = x.permute((0,2,1))\n",
    "        #shape of x: [batch_size, embedding_dim, maxlen]\n",
    "        x = self.conv1(x)\n",
    "        #shape of x: [batch_size, filters, maxlen]\n",
    "        x = F.relu(x)\n",
    "        #shape of x: [batch_size, filters, maxlen]\n",
    "        x = self.max_pooling(x)\n",
    "        #shape of x: [batch_size, filters, 1]\n",
    "        x = x.squeeze()\n",
    "        #shape of x: [batch_size, filters]\n",
    "        x = self.output_layer(x)\n",
    "        #shape of x: [batch_size, 2]\n",
    "        x = self.softmax(x)\n",
    "        #shape of x: [batch_size, 2]\n",
    "        return x\n",
    "\n",
    "model = Model(embedding_dims, max_features, filters, kernel_size, maxlen)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#adapted from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "\n",
    "        inputs = x_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #Calculate gradients\n",
    "        loss.backward()\n",
    "        #Let optimizer update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0 and i > 0:    # print every 100\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    #Let's calculate accuracy after an epoch\n",
    "    answers = []\n",
    "    for i in range(0, len(x_test), batch_size):\n",
    "        xx = model(x_test[i:i+batch_size])\n",
    "        #2 dimensional output to 1d class vector\n",
    "        values, indices = torch.max(xx, dim=1)\n",
    "        answers.extend((indices == torch.tensor(y_test[i:i+batch_size])).tolist())\n",
    "    print ('acc', answers.count(True)/len(answers))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_text_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

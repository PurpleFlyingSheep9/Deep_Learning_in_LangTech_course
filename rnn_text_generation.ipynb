{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN text generation\n",
    "\n",
    "In this notebook, we'll have a look at simple character-level text generation with recurrent neural networks in Keras.\n",
    "\n",
    "This is largely a simplified version of the Tensorflow tutorial [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation), which in turn follows in part Andrej Karpathy's\n",
    " [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Shakespeare dataset\n",
    "\n",
    "We'll use a dataset of texts from Shakespeare provided by Google. The approach would work with any reasonably-sized plain text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘shakespeare.txt’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n",
    "!wget -nc https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple plain text file, so we'll just read it in directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "with open('shakespeare.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the text includes punctuation and newline characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(repr(text[:200]))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text\n",
    "\n",
    "First, find the set of unique characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(set(text))\n",
    "\n",
    "# What did we get?\n",
    "print(len(characters))\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mapping from characters to integers and the inverse mapping from those integers back to the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = { c: i for i, c in enumerate(characters) }\n",
    "int_to_char = { i: c for c, i in char_to_int.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at that mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0,\n",
      " ' ': 1,\n",
      " '!': 2,\n",
      " '$': 3,\n",
      " '&': 4,\n",
      " \"'\": 5,\n",
      " ',': 6,\n",
      " '-': 7,\n",
      " '.': 8,\n",
      " '3': 9,\n",
      " ':': 10,\n",
      " ';': 11,\n",
      " '?': 12,\n",
      " 'A': 13,\n",
      " 'B': 14,\n",
      " 'C': 15,\n",
      " 'D': 16,\n",
      " 'E': 17,\n",
      " 'F': 18,\n",
      " 'G': 19}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint    # pretty-printer\n",
    "\n",
    "\n",
    "def truncate_dict(d, count=10):\n",
    "    # Returns at most count items from the given dictionary.  \n",
    "    return dict(i for i, _ in zip(d.items(), range(count)))\n",
    "\n",
    "\n",
    "pprint(truncate_dict(char_to_int, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map text characters to the (arbitrary) indices defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59]\n"
     ]
    }
   ],
   "source": [
    "data = [char_to_int[c] for c in text]\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split that data into fixed-length parts for training.\n",
    "\n",
    "(The +1 is here because we'll be shifting the outputs by one character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 100\n",
    "\n",
    "sequences = []\n",
    "for i in range(0, len(data), sequence_length+1):\n",
    "    sequences.append(data[i:i+sequence_length+1])\n",
    "sequences.pop()    # drop (likely) incomplete last sequence\n",
    "\n",
    "    \n",
    "for i in range(5):\n",
    "    print(repr(''.join(int_to_char[c] for c in sequences[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into paired inputs and outputs so that each output matches the text of its input, shifted to the right by one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = [], []\n",
    "for sequence in sequences:\n",
    "    inputs.append(sequence[:-1])\n",
    "    outputs.append(sequence[1:])\n",
    "    \n",
    "    \n",
    "print(repr(''.join(int_to_char[c] for c in inputs[0])))\n",
    "print(repr(''.join(int_to_char[c] for c in outputs[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "We build a straightforward RNN model where each RNN state outputs unnormalized predictions (\"logits\"):\n",
    "\n",
    "* input: sequence of `sequence_length` integers corresponding to characters\n",
    "* embedding: randomly initialized mapping to `embedding_dim`-dimensional vectors\n",
    "* rnn: recurrent neural network with `rnn_units`-dimensional state\n",
    "* output: `num_classes`-dimensional fully connected layer with softmax activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/smp/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/smp/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 256)          16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100, 1024)         3935232   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100, 65)           66625     \n",
      "=================================================================\n",
      "Total params: 4,018,497\n",
      "Trainable params: 4,018,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense\n",
    "\n",
    "\n",
    "vocab_size = len(characters)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "\n",
    "input_ = Input(shape=(sequence_length,))\n",
    "embedding = Embedding(vocab_size, embedding_dim)(input_)\n",
    "# Note: return_sequences=True\n",
    "rnn = GRU(rnn_units, return_sequences=True)(embedding)\n",
    "# Note: no activation function (e.g. softmax)\n",
    "output = Dense(vocab_size)(rnn)\n",
    "model = Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an appropriate loss function and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "\n",
    "# This is just 'sparse_categorical_crossentropy' where from_logits=True\n",
    "# specifies that the values are unnormalized (no softmax)\n",
    "def sparse_categorical_crossentropy_with_logits(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=sparse_categorical_crossentropy_with_logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast inputs and outputs as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11043 100\n",
      "(11043, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X, Y = np.array(inputs), np.array(outputs)\n",
    "print(len(inputs), len(inputs[0]))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. (This will take a while unless running on a GPU.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text with trained model\n",
    "\n",
    "Note that as we set a fixed `sequence_length` to keep things simple, we won't actually be using the RNN state here; we're instead simply always giving the RNN the catenation of the initial seed text and previously generated characters  as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def predict_one_character(input_, temperature=1.0):\n",
    "    input_ = ' ' * (sequence_length-len(input_)) + input_   # pad to sequence_length\n",
    "    input_ = input_[-sequence_length:]    # truncate if longer\n",
    "    X = np.array([[char_to_int[c] for c in input_]])    # note batch dim\n",
    "    model.reset_states()    # forget state\n",
    "    Y = model(X)\n",
    "    Y = tf.squeeze(Y, 0)    # drop batch dim\n",
    "    Y = Y / temperature\n",
    "    # Sample from categorical distribution\n",
    "    pred_id = tf.random.categorical(Y, num_samples=1)[-1,0].numpy()\n",
    "    return int_to_char[pred_id]\n",
    "\n",
    "\n",
    "seed = 'ROMEO'\n",
    "string = seed\n",
    "generated = []\n",
    "for i in range(500):\n",
    "    # Lower temperature gives more likely predictions, higher more surprising\n",
    "    char = predict_one_character(string, temperature=1.0)\n",
    "    generated.append(char)\n",
    "    string += char\n",
    "\n",
    "\n",
    "print(seed + ''.join(generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

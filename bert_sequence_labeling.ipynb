{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence labeling with BERT\n",
    "\n",
    "This notebook briefly demonstrates fine-tuning a pretrained BERT model to a sequence labeling task.\n",
    "\n",
    "You probably want to run this notebook with GPU acceleration, as fine-tuning BERT on CPU can be fairly slow even with a comparatively small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The following variables configure a few aspects of the data, model, and training process. To adapt this example to a different dataset, you'll probably want to change these to match.\n",
    "\n",
    "Note in particular that we're limiting the number of examples and the maximum sequence length to make training faster. \n",
    "\n",
    "When running on GPU, it's also necessary to make sure that the input length and batch size are not so large as to cause a batch to exceed GPU memory. If you're getting a message like `Resource exhausted: OOM when allocating tensor`, try smaller numbers for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum number of examples to read\n",
    "MAX_EXAMPLES = 2000\n",
    "\n",
    "# Maximum length of input sequence in tokens\n",
    "INPUT_LENGTH = 25\n",
    "\n",
    "# Number of epochs to train for\n",
    "EPOCHS = 3\n",
    "\n",
    "# Optimizer learning rate\n",
    "LEARNING_RATE = 0.00002\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package setup\n",
    "\n",
    "We'll use [keras-bert](https://github.com/CyberZHG/keras-bert). Make sure the package is installed. (`pip` is the Python [package installer](https://pip.pypa.io/en/stable/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-bert in /Users/smp/Library/Python/3.7/lib/python/site-packages (0.81.0)\n",
      "Requirement already satisfied: Keras in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-bert) (2.3.1)\n",
      "Requirement already satisfied: numpy in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-bert) (1.18.1)\n",
      "Requirement already satisfied: keras-transformer>=0.30.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-bert) (0.32.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (1.14.0)\n",
      "Requirement already satisfied: pyyaml in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (5.3)\n",
      "Requirement already satisfied: h5py in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (2.10.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from Keras->keras-bert) (1.4.1)\n",
      "Requirement already satisfied: keras-multi-head>=0.22.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.22.0)\n",
      "Requirement already satisfied: keras-layer-normalization>=0.12.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.14.0)\n",
      "Requirement already satisfied: keras-embed-sim>=0.7.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.7.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.6.0)\n",
      "Requirement already satisfied: keras-pos-embd>=0.10.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-transformer>=0.30.0->keras-bert) (0.11.0)\n",
      "Requirement already satisfied: keras-self-attention==0.41.0 in /Users/smp/Library/Python/3.7/lib/python/site-packages (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert) (0.41.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow configuration\n",
    "\n",
    "We'll need set an environment variable for keras-bert to use `tensorflow.python.keras`. (This is a technical detail that is not related to the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ['TF_KERAS'] = '1'    # Required to use tensorflow.python.keras with keras-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "We'll use part-of-speech tags from a recent [Universal Dependencies]() version of the English [EWT corpus](https://nlp.stanford.edu/pubs/Gold_LREC14.pdf).\n",
    "\n",
    "Download the training and development subsets of the corpus data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘en_ewt-ud-train.conllu’ already there; not retrieving.\n",
      "\n",
      "File ‘en_ewt-ud-dev.conllu’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n",
    "!wget -nc https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-train.conllu\n",
    "!wget -nc https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The corpus is in [CoNLL-U](https://universaldependencies.org/format) format. We won't get into the details of the format here, as it's enough to know the following:\n",
    "\n",
    "* Lines beginning with `#` are comments and can be ignored\n",
    "* Empty lines delimit sentences\n",
    "* Remaining lines contain information on words, with fields separated by a tab\n",
    "* On lines where the first field is an integer (word index), the second field contains the word surface form, and the fourth the part-of-speech tag\n",
    "\n",
    "We'll grab the wordforms and POS tags and keep them organized by sentences.\n",
    "\n",
    "Note that before (potentially) truncating the examples to `MAX_EXAMPLES` we'll shuffle the list of sentences, but keep the words in their original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: truncating examples in en_ewt-ud-train.conllu from 12543 to 2000\n",
      "Note: truncating examples in en_ewt-ud-dev.conllu from 2002 to 2000\n",
      "\n",
      "Example sentence:\n",
      "They\tPRON\n",
      "had\tVERB\n",
      "a\tDET\n",
      "great\tADJ\n",
      "selection\tNOUN\n",
      "of\tADP\n",
      "colors\tNOUN\n",
      "to\tPART\n",
      "choose\tVERB\n",
      "from\tADP\n",
      "and\tCCONJ\n",
      "their\tPRON\n",
      "seats\tNOUN\n",
      "are\tAUX\n",
      "super\tADV\n",
      "comfty\tADJ\n",
      ".\tPUNCT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "random.seed(1234)    # This makes random.shuffle() produce the same order every time\n",
    "\n",
    "\n",
    "\n",
    "def load_conllu_tags(fn):\n",
    "    sentences = []\n",
    "    with open(fn) as f:\n",
    "        current = []\n",
    "        for line in f:\n",
    "            line = line.rstrip('\\n')\n",
    "            if line.startswith('#'):\n",
    "                continue    # Comment line, skip\n",
    "            if line == '' or line.isspace():\n",
    "                # Empty line, sentence boundary\n",
    "                sentences.append(current)\n",
    "                current = []\n",
    "                continue\n",
    "            fields = line.split('\\t')\n",
    "            if not fields[0].isdigit():\n",
    "                continue    # Not a regular word, skip\n",
    "            word, tag = fields[1], fields[3]\n",
    "            current.append((word, tag))\n",
    "            \n",
    "    random.shuffle(sentences)\n",
    "    if len(sentences) > MAX_EXAMPLES:\n",
    "        print('Note: truncating examples in {} from {} to {}'.format(fn, len(sentences), MAX_EXAMPLES))\n",
    "        sentences = sentences[:MAX_EXAMPLES]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "train_sentences = load_conllu_tags('en_ewt-ud-train.conllu')\n",
    "dev_sentences = load_conllu_tags('en_ewt-ud-dev.conllu')\n",
    "\n",
    "\n",
    "# Have a look at the data\n",
    "print('\\nExample sentence:')\n",
    "for word, tag in train_sentences[0]:\n",
    "    print('{}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained BERT model\n",
    "\n",
    "As training BERT from scratch generally takes days, we'll here load a pretrained model and fine-tune it for our task. URLs to download pre-trained models made available by Google are found at https://github.com/google-research/bert .\n",
    "\n",
    "`cased_L-12_H-768_A-12` is a case-sensitive BERT \"base\" model for English: 12 layers, 768-dimensional hidden state, and 12 \"heads\" for multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘cased_L-12_H-768_A-12.zip’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Give -nc (--no-clobber) argument so that the file isn't downloaded multiple times \n",
    "!wget -nc https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack the downloaded file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  cased_L-12_H-768_A-12.zip\r\n"
     ]
    }
   ],
   "source": [
    "# Give -n argument so that existing files aren't overwritten \n",
    "!unzip -n cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key parts to the package contents:\n",
    "\n",
    "* `vocab.txt`: plain text file listing vocabulary items\n",
    "* `bert_config.json`: model configuration in JSON format\n",
    "* `bert_model.ckpt.*`: model checkpoint data with pre-trained weights in [Tensorflow checkpoint format](https://www.tensorflow.org/guide/checkpoint)\n",
    "\n",
    "Take note of the path to these (you'll need to change this if you pick a different model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_path = 'cased_L-12_H-768_A-12/vocab.txt'\n",
    "bert_config_path = 'cased_L-12_H-768_A-12/bert_config.json'\n",
    "bert_checkpoint_path = 'cased_L-12_H-768_A-12/bert_model.ckpt'    # suffixes not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also take note if the model we downloaded was a case-sensitive (cased) or not. (This must match the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_is_cased = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT vocabulary\n",
    "\n",
    "This is just a plain text file with one vocabulary item per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', 'щ', '吉', 'told', 'space', 'operations', 'proposed', 'Oxford', 'showing', 'domestic', 'mountains', 'commission', 'voices', 'associate', 'hills', 'Guide', 'relaxed', 'Page', 'Heights', 'singers', 'Interior', 'considers', 'facilitate', 'shouting', '1826', 'constitute', 'alter', 'clip', 'Into', 'Memory', 'ballad', 'Owens', 'Langdon', 'aquatic', 'stereo', 'Cass', 'Shock', '195', '##tec', '##sonic', 'attested', '##rdes', '1840s', '##90', 'Guys', '##rien', 'Munro', 'Ursula', 'mesh', 'diplomacy', 'Newmarket', '##oughs', 'synthesizers', 'Drugs', 'monstrous', '##ynamic', 'troll', '##ٹ']\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "with open(bert_vocab_path) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vocab.append(line.rstrip('\\n'))    # rstrip to remove newline characters\n",
    "\n",
    "\n",
    "# Print a list with every 500th vocabulary item\n",
    "print(vocab[0::500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT configuration\n",
    "\n",
    "The configuration is just a JSON file, so we can read it in with `json.load` from the python `json` library.\n",
    "\n",
    "We won't actually need to use these configuration details directly (keras-bert takes care of them for us), so this is just here to show what information is contained in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_probs_dropout_prob': 0.1,\n",
      " 'hidden_act': 'gelu',\n",
      " 'hidden_dropout_prob': 0.1,\n",
      " 'hidden_size': 768,\n",
      " 'initializer_range': 0.02,\n",
      " 'intermediate_size': 3072,\n",
      " 'max_position_embeddings': 512,\n",
      " 'num_attention_heads': 12,\n",
      " 'num_hidden_layers': 12,\n",
      " 'type_vocab_size': 2,\n",
      " 'vocab_size': 28996}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pprint import pprint    # pretty-printer for output\n",
    "\n",
    "\n",
    "with open(bert_config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "\n",
    "# Print configuration contents\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BERT tokenizer\n",
    "\n",
    "To create the tokenizer, we'll need a mapping from vocabulary items to their corresponding integer indices. We do this conventionally using `enumerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'##umi': 14088,\n",
      " 'Helsinki': 12471,\n",
      " 'assassinated': 17493,\n",
      " 'brackets': 22019,\n",
      " 'drowned': 14100,\n",
      " 'educational': 4339,\n",
      " 'firmly': 7487,\n",
      " 'instincts': 17477,\n",
      " 'internet': 7210,\n",
      " 'nuts': 13937}\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from vocabulary items to their indices in the vocabulary\n",
    "token_dict = { v: i for i, v in enumerate(vocab) }\n",
    "\n",
    "\n",
    "# Print some random examples of the mapping\n",
    "pprint(dict(random.choices(list(token_dict.items()), k=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the keras-bert `Tokenizer` for BERT tokenization. The implementation supports\n",
    "\n",
    "* (Optional) lowercasing: `Hello` → `hello`\n",
    "* Basic tokenization: `Hello!` → `Hello` `!`, `multi-part` → `multi` `-` `part`\n",
    "* Wordpiece tokenization: `comprehensively` → `comprehensive` `##ly`\n",
    "* Adding special tokens: `Sentence`  → `[CLS]` `Sentence` `[SEP]`\n",
    "* Mapping to integer indices\n",
    "* Generating segment sequence\n",
    "* (Optional) padding and truncation to length\n",
    "\n",
    "In the following example, notice how words not in the dictionary are broken up into subwords (with continuation parts starting with `##`) and how unknown _characters_ are mapped to a special unknown word token `[UNK]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: Hello BERT!\n",
      "Tokenized: ['[CLS]', 'Hello', 'B', '##ER', '##T', '!', '[SEP]']\n",
      "Encoded: [101, 8667, 139, 9637, 1942, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded: Hello B ##ER ##T !\n",
      "\n",
      "Original string: Unknown: 你\n",
      "Tokenized: ['[CLS]', 'Unknown', ':', '你', '[SEP]']\n",
      "Encoded: [101, 16285, 131, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Segments: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded: Unknown : [UNK]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(token_dict, cased=model_is_cased)\n",
    "\n",
    "\n",
    "# Let's test that out\n",
    "for s in ['Hello BERT!', 'Unknown: 你']:\n",
    "    print('Original string:', s)\n",
    "    print('Tokenized:', tokenizer.tokenize(s))\n",
    "    indices, segments = tokenizer.encode(s, max_len=20)    # max_len for padding and truncation\n",
    "    print('Encoded:', indices)\n",
    "    print('Segments:', segments)\n",
    "    print('Decoded:', ' '.join(tokenizer.decode(indices)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize data\n",
    "\n",
    "We'll use keras-bert `Tokenizer` to tokenize and vectorize words. For vectorizing labels (tags), we'll create a simple mapping ourselves.\n",
    "\n",
    "We'll also add a special placeholder label for padding and continuation wordpieces (more on that below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 18\n",
      "Tags: ['NONE', 'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
      "Mapping: {'NONE': 0, 'ADJ': 1, 'ADP': 2, 'ADV': 3, 'AUX': 4, 'CCONJ': 5, 'DET': 6, 'INTJ': 7, 'NOUN': 8, 'NUM': 9, 'PART': 10, 'PRON': 11, 'PROPN': 12, 'PUNCT': 13, 'SCONJ': 14, 'SYM': 15, 'VERB': 16, 'X': 17}\n",
      "Inverted: {0: 'NONE', 1: 'ADJ', 2: 'ADP', 3: 'ADV', 4: 'AUX', 5: 'CCONJ', 6: 'DET', 7: 'INTJ', 8: 'NOUN', 9: 'NUM', 10: 'PART', 11: 'PRON', 12: 'PROPN', 13: 'PUNCT', 14: 'SCONJ', 15: 'SYM', 16: 'VERB', 17: 'X'}\n"
     ]
    }
   ],
   "source": [
    "# This is our special placeholder label value\n",
    "NO_LABEL = 'NONE'\n",
    "\n",
    "\n",
    "# This just flattens out tags from the sentences list-of-lists\n",
    "train_tags = [tag for sentence in train_sentences for word, tag in sentence]\n",
    "# Use set() to get the unique tags and add our special label to the list\n",
    "unique_tags = [NO_LABEL] + sorted(set(train_tags))\n",
    "\n",
    "# Create mappings from tags to integer values and back\n",
    "tag_to_int = { t: i for i, t in enumerate(unique_tags) }\n",
    "int_to_tag = { i: t for t, i in tag_to_int.items() }\n",
    "\n",
    "# Take note of how many unique labels (tags) there are in the data\n",
    "num_labels = len(unique_tags)\n",
    "\n",
    "\n",
    "# Let's see what we got\n",
    "print('Number of unique labels:', num_labels)\n",
    "print('Tags:', unique_tags)\n",
    "print('Mapping:', tag_to_int)\n",
    "print('Inverted:', int_to_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source data has one tag for each (syntactic) word. If we were training a model from scratch, we could define the tokenization to match the data so that we would get a simply one-to-one mapping between input words and output tags. However, as we're using a pre-trained model, we need to work with the existing tokenization, which may split up some of the input words into multiple parts.\n",
    "\n",
    "For example, if the data contains the word, tag pair `(complicatedly, ADV)` and the tokenizer splits up the word into `complicated` `##ly`, we have two tokens (`complicated` and `##ly`) but just one tag (`ADV`). There are a number of ways we could resolve this. Here, we will assign the original tag to the first wordpiece and the placeholder tag we introduced earlier to any subsequent pieces, so e.g. `(complicatedly, ADV)` would map to `(complicated, ADV)`, `(##ly, NONE)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They'] ['PRON']\n",
      "['had'] ['VERB']\n",
      "['a'] ['DET']\n",
      "['great'] ['ADJ']\n",
      "['selection'] ['NOUN']\n",
      "['of'] ['ADP']\n",
      "['colors'] ['NOUN']\n",
      "['to'] ['PART']\n",
      "['choose'] ['VERB']\n",
      "['from'] ['ADP']\n",
      "['and'] ['CCONJ']\n",
      "['their'] ['PRON']\n",
      "['seats'] ['NOUN']\n",
      "['are'] ['AUX']\n",
      "['super'] ['ADV']\n",
      "['com', '##fty'] ['ADJ', 'NONE']\n",
      "['.'] ['PUNCT']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize_word(word):\n",
    "    # keras-bert tokenizer wraps its inputs with the special BERT tokens\n",
    "    # [CLS] and [SEP]. To tokenize individual words, we remove these.\n",
    "    wrapped = tokenizer.tokenize(word)\n",
    "    return wrapped[1:-1]    # remove [CLS] at the first and [SEP] at the last position\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokenized = []\n",
    "    for word, tag in sentence:\n",
    "        tokens = tokenize_word(word)\n",
    "        # Original tag for first wordpiece, fill in placeholder tags for\n",
    "        # any continuation pieces.\n",
    "        tags = [tag] + [NO_LABEL] * (len(tokens)-1)\n",
    "        tokenized.append((tokens, tags))\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    return [tokenize_sentence(s) for s in sentences]\n",
    "\n",
    "\n",
    "tokenized_train = tokenize_sentences(train_sentences)\n",
    "tokenized_dev = tokenize_sentences(dev_sentences)\n",
    "\n",
    "\n",
    "# Let's look at an example sentence after tokenization\n",
    "for tokens, tags in tokenized_train[0]:\n",
    "    print(tokens, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize a tokenized sentence and its labels, we'll truncate to target length, add the special BERT tokens `[CLS]` and `[SEP]` (and corresponding placeholder labels), pad if necessary, and map both tokens and tags to integer indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X[0].shape: (2000, 25)\n",
      "train_X[1].shape: (2000, 25)\n",
      "train_Y[0].shape: (25, 18)\n",
      "train_X[0][0] (first sentence token IDs):\n",
      "[  101  1220  1125   170  1632  4557  1104  5769  1106  4835  1121  1105\n",
      "  1147  3474  1132  7688  3254 27944   119   102     0     0     0     0\n",
      "     0]\n",
      "train_X[1][0] (first sentence segment IDs):\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_Y[0][0] (first sentence tag IDs):\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "decoded train_X[0][0] (first sentence tokens):\n",
      "[CLS] They had a great selection of colors to choose from and their seats are super com ##fty . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "decoded train_Y[0] (first sentence tags):\n",
      "NONE PRON VERB DET ADJ NOUN ADP NOUN PART VERB ADP CCONJ PRON NOUN AUX ADV ADJ NONE PUNCT NONE NONE NONE NONE NONE NONE\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def vectorize_tokenized(tokenized, length=INPUT_LENGTH):\n",
    "    # Flatten lists-of-lists of tokens and tags\n",
    "    token_sequence, tag_sequence = [], []\n",
    "    for tokens, tags in tokenized:\n",
    "        token_sequence.extend(tokens)\n",
    "        tag_sequence.extend(tags)\n",
    "\n",
    "    # Truncation and padding using length-2 to leave space for\n",
    "    # wrapping with the special [CLS] and [SEP] tokens\n",
    "    if len(token_sequence) > length-2:\n",
    "        token_sequence = token_sequence[:length-2]\n",
    "        tag_sequence = tag_sequence[:length-2]\n",
    "\n",
    "    # Wrap with [CLS] and [SEP], adding corresponding placeholder labels\n",
    "    token_sequence = ['[CLS]'] + token_sequence + ['[SEP]']\n",
    "    tag_sequence = [NO_LABEL] + tag_sequence + [NO_LABEL]\n",
    "\n",
    "    while len(token_sequence) < length:\n",
    "        token_sequence.append('[PAD]')\n",
    "        tag_sequence.append(NO_LABEL)\n",
    "    \n",
    "    # Use keras-bert tokenizer and previously created label mapping\n",
    "    token_ids = tokenizer._convert_tokens_to_ids(token_sequence)\n",
    "    tag_ids = [tag_to_int[tag] for tag in tag_sequence]\n",
    "    \n",
    "    # Also create all-zeros segment IDs for BERT\n",
    "    segment_ids = [0] * INPUT_LENGTH\n",
    "\n",
    "    return token_ids, segment_ids, tag_ids\n",
    "\n",
    "\n",
    "def vectorize_dataset(tokenized_data, length=INPUT_LENGTH):\n",
    "    # Create separate lists of token ID, segment ID, and tag ID lists,\n",
    "    # each of the inner lists representing one sentence.\n",
    "    token_ids_list, segment_ids_list, tag_ids_list = [], [], []\n",
    "    for tokenized in tokenized_data:\n",
    "        token_ids, segment_ids, tag_ids = vectorize_tokenized(tokenized, length=length)\n",
    "        token_ids_list.append(token_ids)\n",
    "        segment_ids_list.append(segment_ids)\n",
    "        tag_ids_list.append(tag_ids)\n",
    "\n",
    "    # Return as numpy arrays. Input (X) consists of the token and\n",
    "    # segment IDs, output (Y) of the tag ids. We'll use a one-hot\n",
    "    # representation for the output.\n",
    "    X = [np.array(token_ids_list), np.array(segment_ids_list)]\n",
    "    Y = to_categorical(np.array(tag_ids_list))\n",
    "\n",
    "    return X, Y\n",
    "    \n",
    "\n",
    "train_X, train_Y = vectorize_dataset(tokenized_train)\n",
    "dev_X, dev_Y = vectorize_dataset(tokenized_dev)\n",
    "\n",
    "\n",
    "# Let's have a bit of a look at that\n",
    "print('train_X[0].shape:', train_X[0].shape)\n",
    "print('train_X[1].shape:', train_X[1].shape)\n",
    "print('train_Y[0].shape:', train_Y[0].shape)\n",
    "print('train_X[0][0] (first sentence token IDs):')\n",
    "print(train_X[0][0])\n",
    "print('train_X[1][0] (first sentence segment IDs):')\n",
    "print(train_X[1][0])\n",
    "print('train_Y[0][0] (first sentence tag IDs):')\n",
    "print(train_Y[0])\n",
    "print('decoded train_X[0][0] (first sentence tokens):')\n",
    "print(' '.join([tokenizer._token_dict_inv[t] for t in train_X[0][0]]))\n",
    "print('decoded train_Y[0] (first sentence tags):')\n",
    "print(' '.join([int_to_tag[t.argmax()] for t in train_Y[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained BERT model\n",
    "\n",
    "We'll use the keras-bert function `load_trained_model_from_checkpoint` to load the model from the checkpoint we downloaded earlier.\n",
    "\n",
    "Explanation for a few parameters from keras-bert documentation:\n",
    "\n",
    "* `training`: If `training`, the whole model will be returned. Otherwise, the MLM [masked language modeling] and NSP [next sentence prediction] parts will be ignored.\n",
    "* `trainable`: Whether the model is trainable. The default value is the same with `training`.\n",
    "\n",
    "We don't need the masked language modeling or next sentence prediction parts (these are primarily for pretraining), so we'll use `training=False` but `trainable=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/smp/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/smp/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "\n",
    "\n",
    "pretrained_model = load_trained_model_from_checkpoint(\n",
    "    config_file = bert_config_path,\n",
    "    checkpoint_file = bert_checkpoint_path,\n",
    "    training = False,\n",
    "    trainable = True,\n",
    "    seq_len = INPUT_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a bit of a look at that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Input-Token:0' shape=(?, 25) dtype=float32>,\n",
       " <tf.Tensor 'Input-Segment:0' shape=(?, 25) dtype=float32>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a keras model, so we can figure out what inputs it takes like so:\n",
    "pretrained_model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Encoder-12-FeedForward-Norm/add_1:0' shape=(?, 25, 768) dtype=float32>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And similarly for outputs:\n",
    "pretrained_model.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the model takes `Input-Token` and `Input-Segment` inputs, both of dimension (batch-size, input-length), and produces a single output tensor of dimension (batch-size, input-length, hidden-dim). This is just what we want for sequence labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 25, 768), (2 22268928    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 25, 768)      1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 25, 768)      0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 25, 768)      19200       Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 25, 768)      0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 25, 768)      1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 25, 768)      2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 25, 768)      0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 25, 768)      2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 25, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 25, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 25, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 25, 768)      0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 25, 768)      0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 25, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 25, 768)      2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 25, 768)      1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 25, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 25, 768)      0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 25, 768)      0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 25, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 25, 768)      2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 25, 768)      1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 25, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 25, 768)      0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 25, 768)      0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 25, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 25, 768)      2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 25, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 25, 768)      1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 25, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 25, 768)      0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 25, 768)      0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 25, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n",
      "==================================================================================================\n",
      "Total params: 107,345,664\n",
      "Trainable params: 107,345,664\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a regular Keras model. In Keras, models behave very much like layers, so we're able to wrap this in our own model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build classification model\n",
    "\n",
    "We'll make a _very_ simple model for sequence labeling: just attach a time-distributed dense layer with softmax activation to the output, and connect the model inputs to the BERT model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Encoder-12-FeedForward-Norm/add_1:0\", shape=(?, 25, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# model.outputs is a list, here with a single item. We'll\n",
    "# add our output layer on top of that.\n",
    "bert_out = pretrained_model.outputs[0]\n",
    "\n",
    "print(bert_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our model. This is just basic Keras, where the pretrained BERT model is behaving essentially as a layer of our \"wrapping\" model.\n",
    "\n",
    "Note that input sequences shorter than `INPUT_LENGTH` have been padded to that length and placeholder labels have been added to words consisting of more than one subword token. The model could be made to ignore these irrelevant parts of the data e.g. by masking, but we'll keep things simple here and include them without changing the model or training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense\n",
    "\n",
    "\n",
    "out = TimeDistributed(Dense(num_labels, activation='softmax'))(bert_out)\n",
    "model = Model(\n",
    "    inputs=pretrained_model.inputs,\n",
    "    outputs=[out]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create optimizer\n",
    "\n",
    "BERT is pretrained with an Adam optimizer with warmup and regularization using weight decay. We won't go into detail on these optimizer settings, but will instead largely copy parameters used in the original BERT work.\n",
    "\n",
    "(If you're interested in tuning the training process, trying different values of `LEARNING_RATE` is a good place to start!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import calc_train_steps, AdamWarmup\n",
    "\n",
    "\n",
    "# Calculate the number of steps for warmup\n",
    "total_steps, warmup_steps = calc_train_steps(\n",
    "    num_example=len(train_sentences),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    warmup_proportion=0.1,\n",
    ")\n",
    "\n",
    "optimizer = AdamWarmup(\n",
    "    total_steps,\n",
    "    warmup_steps,\n",
    "    lr=LEARNING_RATE,\n",
    "    epsilon=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    weight_decay_pattern=['embeddings', 'kernel', 'W1', 'W2', 'Wk', 'Wq', 'Wv', 'Wo']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "The model is compiled and trained normally. As usual, we'll use `sparse_categorical_crossentropy` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, as usual. (Note: this will take a fair bit of time unless you're running with GPU acceleration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/smp/Library/Python/3.7/lib/python/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "2000/2000 [==============================] - 977s 488ms/sample - loss: 0.6824 - acc: 0.7081 - val_loss: 0.1048 - val_acc: 0.9489\n",
      "Epoch 2/3\n",
      "2000/2000 [==============================] - 1096s 548ms/sample - loss: 0.0861 - acc: 0.9633 - val_loss: 0.0798 - val_acc: 0.9610\n",
      "Epoch 3/3\n",
      "2000/2000 [==============================] - 1144s 572ms/sample - loss: 0.0493 - acc: 0.9801 - val_loss: 0.0768 - val_acc: 0.9626\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_X,\n",
    "    train_Y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=[dev_X, dev_Y]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot that training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dfZzNdf7/8cdrroxrM4YUiVpiinExi1BIrkp0IRmVi5Io9Utls5stbG3tdt3WTkkqfZnJRUXlIsKqpAxCWBdJDFYMhjHXM+/fH58zx5nrM5xzPnPOvO6329zmfC7P63zmzHPe8/58zvsjxhiUUkoFriC7C1BKKeVdGvRKKRXgNOiVUirAadArpVSA06BXSqkAF2J3AUVFRUWZZs2a2V2GUkr5lY0bNx43xjQoaVmlC/pmzZqRlJRkdxlKKeVXROS30pZp141SSgU4DXqllApwGvRKKRXgNOiVUirAadArpVSA06BXSqkAp0GvlFIBrtJdR6+UUgHNGMg8BWdTID0F0o9b388eh+oREDva40+pQa+UUhciN7t4YKenFH/sOm3ySt5Xk04a9Eop5VXGQNZpRyCfKBLejnnO8HZMZ50uZWcCNSKhRn2oEQWRl8Olnc5N14xyLI9yzKsPYTW88rI06JVSgSsvp/zWdfpxl26UFMjPKXlfIeGOUI60Qjryciuca9Y/F9416jsCPAqq14OgYN++3lJo0Cul/IMxkJ1WuLVdqHWdUrjf+2wKZKWWvr/qEecCOqIZNOlYJKzrn/uqGQWhNUDEZy/XkzTolVL2yMuFjBNltK5LCO+87JL3FRzm6A5xBHO9Di6BHenSVeII9uoREFx14q/qvFKllPcYA9lnC/ddl9jadunzzjxV+v7C654L5XqXwiUxJbS2XbpRwmr5bWvbFzTolVLF5edBxsmyu0aK9nvnZpa8r6DQwq3rRm2Ld48UDe/gUN++3gCnQa9UVZCdXnbruuhJyoyTgCl5X9XqnAvoOpdAozYlB3ZBN0q1OtratpkGvVL+Jj/f8YGboq3tMq7fzkkveV8SXDigG0aX3DXiOh1SzbevV10wDXql7JaTWSSsi16/XSS8M06AyS95X2G1zrWuazWEhq2LtLajCnejhNfT1nYVoEGvlCfl51uX9BW6zK+M67fTT1iXDJZEgqB65LmAbnBlkdZ1Cddwh4b79vUqv6BBr5S7Un6Bw5tLDmvXMC/t4+2hNQp3h0S1KNyXXfSqkvB6EKTjDqoLp0GvlDsO/wSz+kNuhmNGkY+3178CmnYuobXt/Y+3u8MYQ76BfGPINwbjfGx9N/nnluWbwusbID/fdZvi65S4z4LH+e6uX4F9utaQX3gf5a1vim5vStredf8VXL/IsrKPm8s+8+HKRrV59c52Hv/5a9CrKs0Yw9nsPM5k5nAmM9fxVfhx3umj3LV1JPnUZsZlb3AiOIq0oFrkmeBzv6yZhvwMx2Ncf3kzyTfJGJPs3i97SeGQX/L6Bsd0fsmh6fp8yiICQSIECYjjuzUthZZZ067LHesHlb++lLW9QFBQUKnbN6ztnRPdGvTKb+Xm5XM2K4/TzmC2vqdlWY9Pu4S1Na94iKdl5ZYZhGHkMDfsOcKCUhkf9jwHUho6fkEzCv2ylhcgwUFCaJCUHQBuBI4UW79guiBE3AysIvssvL0gFN2HYzqoovssGnLF1xeK779C+yw4bkGl1OyyflWlQa9skZWb5wzctEzXYC4c1gXrnC4hrNOzS+kLdxEaLNQOD6V2eIj1VS2USyNrUDs8hDou82tVc1knPJQ64SHUqhZM1KrHCd26G+74gA+uutUHR0Ypz9OgVxVijCEjJ88ZuKeLhLXr/KJhXdCCPp2ZS3ZuKZcHuqgeGmyFsEv4Xlw3nNrVQh3zQpwhXqeEsK4dHkK1kKDzb8mtj4etc6HHk6Ahr/yYBn0VkpdvnOFbtHV8uoSwPpOZy5ki66Vl5ZJXTqevCNQKKxy49WuF0SyqpjOI64SHUqta4XUKWtwF4R4abOMVJ3u/huV/gVYDocdk++pQygM06P1ETl5+kWDOcQRz6aFcuBvE+l6ekCA514p2hG7jetWpE167WCu6dngotUsI65phIQQF+XF/aMovsGA0NGgNt76jlzgqv+dW0ItIf+B1IBiYaYx5ocjyy4BZQAPgBHC3MSbZsSwP2OZY9YAxZpCHavcLxhgyc/I5k1X8qo40R2AX7doo1tLOyiEzp/yujmohQc4ujoKwblg7vHiruUhY16oW4tgmlPDQC+jqCASZqZAwDIJCIC4BqtWyuyKlLli5QS8iwcBbQB8gGdggIouNMTtcVnsJmG2M+VBErgeeB+5xLMswxnj+wlAfyM83nM3OdQZxWlbhKzmK9k2fLiWsc924vq1WtRCXrowQ6tUIc540LNpyruXS/eEa1mEh2vK8IPl5sOA+OLEPRiyCiMvsrkgpj3CnRd8J2GuM2QcgIonAYMA16KOBxxyPVwOfebLI85Gbl+8MXNdWc1pW4Ss5ip9IdDzOsro6TDkZHSQU68poVCecFg3PtZpdTya6BrPr42B/7uoIFCunwt4VMPBVaNbd7mqU8hh3gr4xcNBlOhnoXGSdLcBtWN07twK1RaS+MSYFCBeRJCAXeMEYU+yPgIiMBcYCNG3atMIvAuB4WhZ3vvO9M6wzcsq/9C4sJKhYH3OzqBrOqzfqhBcPa9f5taqFUCMsuGp3dQSKLYmw7g344/0Qe6/d1SjlUZ46GfsE8KaIjALWAoeAgqS9zBhzSEQuB1aJyDZjzC+uGxtjZgAzAGJjY8/rc3w1woJp1ahO2ddFF+mrrhZSOW7cq2yWnASLH4Fm10L/5+2uRimPcyfoDwGXukw3ccxzMsYcxmrRIyK1gNuNMaccyw45vu8TkTVAe6BQ0HtCjbAQ3rqrg6d3qwLd6cOQeBfUbgRDZ+udjVRAcufs3QaghYg0F5EwYBiw2HUFEYkSkYJ9/RnrChxEJEJEqhWsA3SjcN++UvbJybBCPjsN4hKtQcqUCkDlBr0xJheYACwHdgLzjDHbRWS6iBRcKtkT2CUiu4GLgOcc81sDSSKyBesk7QtFrtZRyh7GWN01hzfDbe/CRdF2V6SU14gp77ISH4uNjTVJSUl2l6EC3bevwcpn4Pq/wnVP2F2NUhdMRDYaY2JLWqYXXquqZ/dy61LKq26Dax+3uxqlvE6DXlUtx3ZZH4q6uC0Mfkvvl6qqBA16VXWkn7CGNwitDsPm2nrHJ6V8SQc1U1VDXq41UFlqMoz6Euo2sbsipXxGg15VDV9NgX1rrO6aSzvZXY1SPqVdNyrwbZoNP8RDl4eg/d12V6OUz2nQq8D22/fwxWNwxfXQZ7rd1ShlCw16FbhOHYSP74Z6TWHILAjWnkpVNWnQq8CUfRYS4yAvxxreoHqE3RUpZRtt4qjAYwx8Nh6Obofh86BBS7srUspWGvQq8Kx9EXYsgr7PQos+dlejlO2060YFlp2fw+rnICYOrplgdzVKVQoa9Cpw/O9n+OQBaBwLA1/T4Q2UctCgV4Hh7HFIiIPwOjBsDoSG212RUpWG9tEr/5ebDfNGwNnfYfQS625RSiknDXrl/5Y9Cb99B7fNhMYd7a5GqUpHu26Uf9swE5JmQfeJ0PYOu6tRqlLSoFf+69e1sPRJaNnfulOUUqpEGvTKP534FeaNhMgrrHu+BgXbXZFSlZYGvfI/WWcgcTiYfIhLsK60UUqVSk/GKv+Snw+fjLVuCXjPJ1D/CrsrUqrS06BX/mX1c7BrCQz4J1ze0+5qlPIL2nWj/MfPC+Gbl6DDCOg01u5qlPIbGvTKPxzeDJ89BE2vgRtf1uENlKoADXpV+Z05Col3Qc0oGPoRhITZXZFSfkX76FXllptl3SUq4yTcuxxqNbC7IqX8jga9qryMgS8mQvKPMHQ2XNzW7oqU8ktudd2ISH8R2SUie0VkcgnLLxORr0Vkq4isEZEmLstGisgex9dITxavAtz6ePhpDvSYDNGD7a5GKb9VbtCLSDDwFjAAiAbiRCS6yGovAbONMW2B6cDzjm0jgWeAzkAn4BkR0Zt3qvLt/Rq+egpa3ww9nrS7GqX8mjst+k7AXmPMPmNMNpAIFG1eRQOrHI9XuyzvB6wwxpwwxpwEVgD9L7xsFdCO74UFo6FhNNzyNgTpNQNKXQh3foMaAwddppMd81xtAW5zPL4VqC0i9d3cFhEZKyJJIpJ07Ngxd2tXgSgzFRKGQVAIDJsL1WrZXZFSfs9TTaUngB4ishnoARwC8tzd2BgzwxgTa4yJbdBAr6qosvLzYMF9cPJX6zLKiMvsrkipgODOVTeHgEtdpps45jkZYw7jaNGLSC3gdmPMKRE5BPQssu2aC6hXBbKVz8DeFdb9Xpt1s7sapQKGOy36DUALEWkuImHAMGCx6woiEiUiBfv6MzDL8Xg50FdEIhwnYfs65ilV2E8JsO5f8Mf7IXa03dUoFVDKDXpjTC4wASugdwLzjDHbRWS6iAxyrNYT2CUiu4GLgOcc254A/ob1x2IDMN0xT6lzDm6Azx+BZtdC/+ftrkapgCPGGLtrKCQ2NtYkJSXZXYbyldOHYUZPCAmHsWugRqTNBSnln0RkozEmtqRl+slYZZ+cDOsGItln4Z7PNOSV8hINemUPY2Dxw3D4J+syyouKfgZPKeUp+kkUZY/vXoNt8+H6KdDqRrurUSqgadAr39u1DFZOg6tvh2sft7sapQKeBr3yrd//CwvHwMUxMOhNvYGIUj6gQa98J/2ENbxBaHWrXz6sht0VKVUl6MlY5Rt5uTB/FJw+BKO+hLrFhjxSSnmJBr3yja+egl//A7fEw6Wd7K5GqSpFu26U9238EH54G66ZAO2G212NUlWOBr3yrt++hy8fhyt6ww3T7K5GqSpJg155z6kD1o29Iy6DIbMgWHsKlbKDBr3yjuyzkDAc8nIgLhGq17O7IqWqLG1iKc/Lz4dPx8Hv22H4PIhqYXdFSlVpGvTK89a+CDsXQ99noUUfu6tRqsrTrhvlWTsWw5q/Q0ycdZWNUsp2GvTKc/63DT59ABrHWrcD1OENlKoUNOiVZ5w9bp18Da8Lw+ZAaLjdFSmlHLSPXl243GyYNwLO/g6jl0LtRnZXpJRyoUGvLowxsHQS/PYd3P4eNO5gd0VKqSK060ZdmA0zYeMH0P0xaDPE7mqUUiXQoFfnb99/YOmT0HIAXP9Xu6tRSpVCg16dnxO/wvyR1oehbpsBQfpWUqqy0t9OVXFZZyAhzuqfj0uA8Dp2V6SUKoOejFUVk58Pn4yF47vhnk8g8nK7K1JKlUODXlXM6mdh1xIY8CJc3tPuapRSbtCuG+W+bQvgm5ehw0jodL/d1Sil3KRBr9xzeDMsegiaXgM3vqTDGyjlRzToVfnOHLWGN6jZAIZ+BCFhdleklKoAt4JeRPqLyC4R2Ssik0tY3lREVovIZhHZKiI3OuY3E5EMEfnJ8fW2p1+A8rLcLPj4Lsg8BcPmQq0GdleklKqgck/Gikgw8BbQB0gGNojIYmPMDpfVpgDzjDHxIhINLAGaOZb9Yoxp59mylU8YA19MhOQNMHQ2XNzW7oqUUufBnRZ9J2CvMWafMSYbSAQGF1nHAAUXU9cFDnuuRGWb9f+Gn+ZAj8kQXfRHrpTyF+4EfWPgoMt0smOeq6nA3SKSjNWaf9hlWXNHl85/ROTakp5ARMaKSJKIJB07dsz96pX37F0JX02B1jdDjyftrkYpdQE8dTI2DvjAGNMEuBH4SESCgCNAU2NMe+AxYK6IFPsYpTFmhjEm1hgT26CB9gHb7vhemH8vNIyGW97W4Q2U8nPu/AYfAi51mW7imOfqPmAegDHmeyAciDLGZBljUhzzNwK/AC0vtGjlRRmnIGEYBIdYJ1+r1bK7IqXUBXIn6DcALUSkuYiEAcOAxUXWOQD0BhCR1lhBf0xEGjhO5iIilwMtgH2eKl55WH4eLLwPTv4Kd/4fRFxmd0VKKQ8o96obY0yuiEwAlgPBwCxjzHYRmQ4kGWMWA48D74rIRKwTs6OMMUZErgOmi0gOkA+MM8ac8NqrURdm5TNW3/zNr8NlXe2uRinlIWKMsbuGQmJjY01SUpLdZVQ9PyXAZ+Og01i48UW7q1FKVZCIbDTGxJa0TM+yKTi4AT5/BJpfB/3+bnc1SikP06Cv6lIPWZ98rXMJ3PEhBIfaXZFSysN0mOKqLCcDEodD9lkYsQhqRNpdkVLKCzToqypjYNEEOLLFuktUw9Z2V6SU8hLtuqmqvn0Vfl4Avf8KVw6wuxqllBdp0FdFu5bC19Ph6tuh+2N2V6OU8jIN+qrm952wcAxcHAOD3tQbiChVBWjQVyXpJ6zhDUJrWMMbhNWwuyKllA/oydiqIi8H5o+E04dh1JdQt+gApEqpQKVBX1Usfwp+XQu3xMOlneyuRinlQ9p1UxVs/AB+fAeumQDthttdjVLKxzToA91v6+DLJ+CK3nDDNLurUUrZQIM+kJ06AB/fYw03PGSWNca8UqrK0aAPVNlnIWG4dRI2LhGq17O7IqWUTbSJF4jy8+HTcfD7drhrPkS1sLsipZSNNOgD0dp/ws7F0Pc5+MMNdlejlLKZdt0Emh2LYM3zEDMcrnnI7mqUUpWABn0g+d82q8umyR9h4Ks6vIFSCtCgDxxpxyAhDsLrWTf2Dg23uyKlVCWhffSBIDcb5o2As8dg9FKo3cjuipRSlYgGvb8zBpY8AQfWwe3vQeMOdleklKpktOvG322YCZs+tMaVbzPE7mqUUpWQBr0/2/cfWPoktBwA1//V7mqUUpWUBr2/OrHPGnY4qgXcNgOC9EeplCqZpoM/yjxtXWFjjHVj7/A6dleklKrE9GSsv8nPh0/GwvE9cM8nEHm53RUppSo5DXp/s/pZ2L0UBrwIl/e0uxqllB9wq+tGRPqLyC4R2Ssik0tY3lREVovIZhHZKiI3uiz7s2O7XSLSz5PFVznbFsA3L0PHUdDpfrurUUr5iXJb9CISDLwF9AGSgQ0istgYs8NltSnAPGNMvIhEA0uAZo7Hw4CrgEuAlSLS0hiT5+kXEvAOb4ZFD0HTrlZrXoc3UEq5yZ0WfSdgrzFmnzEmG0gEBhdZxwAFZwTrAocdjwcDicaYLGPMr8Bex/5URZz5nzW2fM2GcOdHEBJmd0VKKT/iTtA3Bg66TCc75rmaCtwtIslYrfmHK7AtIjJWRJJEJOnYsWNull5F5GRC4l2QeQri5kLNKLsrUkr5GU9dXhkHfGCMaQLcCHwkIm7v2xgzwxgTa4yJbdCggYdKCgDGwBcT4VAS3Po2NGpjd0VKKT/kzlU3h4BLXaabOOa5ug/oD2CM+V5EwoEoN7dVpfn+LdgyF3r+GaKL9pYppZR73Gl1bwBaiEhzEQnDOrm6uMg6B4DeACLSGggHjjnWGyYi1USkOdAC+NFTxQe0PSthxV+h9SC47k92V6OU8mPltuiNMbkiMgFYDgQDs4wx20VkOpBkjFkMPA68KyITsU7MjjLGGGC7iMwDdgC5wEN6xY0bju+BBfdCw6usLhsd3kApdQHEyuPKIzY21iQlJdldhn0yTsHM3tb3sauhXlO7K1JK+QER2WiMiS1pmX4ytjLJz7Na8if3w8jPNeSVUh6hQV+ZrHgafvkabn4dLutqdzVKqQChnb+VxU9z4fs3odNYa4gDpZTyEA36yuDgBvj8/0Hz66Df3+2uRikVYDTo7ZZ6CD6+C+pcAnd8CMGhdleklAow2kdvp5wMSBwO2ekwYhHUiLS7IqVUANKgt4sxsGgCHNli3SWqYWu7K1JKBSjturHLt6/Azwug99Nw5QC7q1FKBTANejv8dwl8/Te4egh0n2h3NUqpAKdB72u/74RP7oeLY2Dwm3oDEaWU12nQ+1L6CUgYBmE1YdhcCK1ud0VKqSpAT8b6Sl4OzBsBpw/DqCVQt9j9V5RSyis06H1l+V9g/zdwSzxc+ke7q1FKVSHadeMLSe/DjzPgmgnQbrjd1SilqhgNem/b/x0seQL+cAP0mW53NUqpKkiD3ptOHYB590BEM7j9PQgKtrsipVQVpEHvLVlpkBAHebkQlwjV69ldkVKqitKTsd6Qnw+fjYffd8Bd8yGqhd0VKaWqMA16b1j7T9i5GPo+Z/XNK6WUjbTrxtN2LII1z0PMcLjmIburUUopDXqPOrIVPh0HTf4IA1/V4Q2UUpWCBr2npB2zxpavHgF3zoHQcLsrUkopQPvoPSM327qM8uwxuHcZ1L7I7oqUUspJg/5CGQNLHocD31vXyl/S3u6KlFKqEO26uVA/vgubZsO1j0ObIXZXo5RSxWjQX4h9a2DZZGg5AHpNsbsapZQqkQb9+TqxD+aNhKiWcNsMCNJDqZSqnNxKJxHpLyK7RGSviEwuYfmrIvKT42u3iJxyWZbnsmyxJ4u3TeZpa3gDEYibC+F17K5IKaVKVe7JWBEJBt4C+gDJwAYRWWyM2VGwjjFmosv6DwOuZyQzjDHtPFeyzfLzrFsBHt8D93wKkZfbXZFSSpXJnRZ9J2CvMWafMSYbSAQGl7F+HJDgieIqpVXPwu5lMOAfcHkPu6tRSqlyuRP0jYGDLtPJjnnFiMhlQHNglcvscBFJEpH1InJLKduNdayTdOzYMTdLt8G2BfDtK9BxFPxxjN3VKKWUWzx9BnEYsMAYk+cy7zJjTCwwHHhNRK4oupExZoYxJtYYE9ugQQMPl+QhhzbBooegaVcY8KIOb6CU8hvuBP0h4FKX6SaOeSUZRpFuG2PMIcf3fcAaCvff+4cz/4PEu6BmQ7jzIwgJs7sipZRymzufjN0AtBCR5lgBPwyrdV6IiLQCIoDvXeZFAOnGmCwRiQK6Af/0ROE+k5NphXzmKbjvK6gZZXdFqpLLyckhOTmZzMxMu0tRASg8PJwmTZoQGhrq9jblBr0xJldEJgDLgWBgljFmu4hMB5KMMQWXTA4DEo0xxmXz1sA7IpKP9d/DC65X61R6xsAXj8KhJBg6Gxq1sbsi5QeSk5OpXbs2zZo1Q7SLT3mQMYaUlBSSk5Np3ry529u5NdaNMWYJsKTIvKeLTE8tYbt1gP+m4/dvwpYE6PlniC7rQiOlzsnMzNSQV14hItSvX5+KXrSiH+cszZ4VsOJpK+Cv+5Pd1Sg/oyGvvOV83lsa9CU5thsW3AsNr4Jb4nV4A6WUX9MEKyrjJCTGQXCYNbxBWE27K1KqQnr16sXy5csLzXvttdcYP358mdvVqlULgMOHDzNkSMkjsfbs2ZOkpKQy9/Paa6+Rnp7unL7xxhs5depUGVt43v79+5k7d65Pn7My06B3lZcLC+6Dk7/Bnf8H9ZraXZFSFRYXF0diYmKheYmJicTFxbm1/SWXXMKCBQvO+/mLBv2SJUuoV6/eee/vfFSWoM/NzbW7BEBvPFLYymfgl6/h5jfgsmvsrkYFgGmfb2fH4dMe3Wf0JXV45uarSl0+ZMgQpkyZQnZ2NmFhYezfv5/Dhw9z7bXXkpaWxuDBgzl58iQ5OTk8++yzDB5c+EKD/fv3M3DgQH7++WcyMjIYPXo0W7ZsoVWrVmRkZDjXGz9+PBs2bCAjI4MhQ4Ywbdo03njjDQ4fPkyvXr2Iiopi9erVNGvWjKSkJKKionjllVeYNWsWAGPGjOHRRx9l//79DBgwgO7du7Nu3ToaN27MokWLqF69eqG65s+fz7Rp0wgODqZu3bqsXbuWvLw8Jk+ezJo1a8jKyuKhhx7igQceYPLkyezcuZN27doxcuRIJk50DsdV5jGYPXs2L730EiJC27Zt+eijjzh69Cjjxo1j3759AMTHx3PJJZc4jxHASy+9RFpaGlOnTqVnz560a9eOb7/9lri4OFq2bMmzzz5LdnY29evXZ86cOVx00UWkpaXx8MMPk5SUhIjwzDPPkJqaytatW3nttdcAePfdd9mxYwevvvrq+b5dAA36czbPsa6y6fQAdBxpdzVKnbfIyEg6derE0qVLGTx4MImJiQwdOhQRITw8nE8//ZQ6depw/PhxunTpwqBBg0o9wRcfH0+NGjXYuXMnW7dupUOHDs5lzz33HJGRkeTl5dG7d2+2bt3KI488wiuvvMLq1auJiir8mZONGzfy/vvv88MPP2CMoXPnzvTo0YOIiAj27NlDQkIC7777LkOHDmXhwoXcfffdhbafPn06y5cvp3Hjxs6uoPfee4+6deuyYcMGsrKy6NatG3379uWFF17gpZde4osvvij2mko7Bjt27ODZZ59l3bp1REVFceLECQAeeeQRevTowaeffkpeXh5paWmcPHmyzJ9Bdna2s4vr5MmTrF+/HhFh5syZ/POf/+Tll1/mb3/7G3Xr1mXbtm3O9UJDQ3nuued48cUXCQ0N5f333+edd94p87ncoUEPcPBH63r55j2g39/trkYFkLJa3t5U0H1TEPTvvfceYF2H/Ze//IW1a9cSFBTEoUOHOHr0KI0aNSpxP2vXruWRRx4BoG3btrRt29a5bN68ecyYMYPc3FyOHDnCjh07Ci0v6ttvv+XWW2+lZk3rvNdtt93GN998w6BBg2jevDnt2lmD3Hbs2JH9+/cX275bt26MGjWKoUOHcttttwHw1VdfsXXrVmdXU2pqKnv27CEsrPRPr5d2DFatWsUdd9zh/AMVGRkJwKpVq5g9ezaA87+J8oL+zjvvdD5OTk7mzjvv5MiRI2RnZzuvf1+5cmWhLraIiAgArr/+er744gtat25NTk4Obdpc+BXqGvSph6xPvtZpDHd8AMF6SJT/Gzx4MBMnTmTTpk2kp6fTsWNHAObMmcOxY8fYuHEjoaGhNGvW7Lw+wfvrr7/y0ksvsWHDBiIiIhg1atQFfRK4WrVqzsfBwcGFuogKvP322/zwww98+eWXdOzYkY0bN2KM4V//+hf9+vUrtO6aNWtKfS5PHIOQkBDy8/Od00W3L/hjBvDwww/z2GOPMWjQINasWcPUqVPL3PeYMWP4+9//TqtWrRg9enSF6ipN1T4Zm50OicMhJwPiEqBGpN0VKeURtWrVolevXtx7772FTsKmpqbSsGFDQkNDWb16Nb/99luZ+7nuuuucJzV//vlntm7dCsDp06epWU8q+6IAABCRSURBVLMmdevW5ejRoyxdutS5Te3atTlz5kyxfV177bV89tlnpKenc/bsWT799FOuvfZat1/TL7/8QufOnZk+fToNGjTg4MGD9OvXj/j4eHJycgDYvXs3Z8+eLbWGso7B9ddfz/z580lJSQFwdt307t2b+Ph4APLy8khNTeWiiy7i999/JyUlhaysrBK7iFyfr3Fja8DfDz/80Dm/T58+vPXWW87pgv8SOnfuzMGDB5k7d67bJ9DLU3WD3hhYPAGObIHb34WGre2uSCmPiouLY8uWLYXC4q677iIpKYk2bdowe/ZsWrVqVeY+xo8fT1paGq1bt+bpp592/mcQExND+/btadWqFcOHD6dbt27ObcaOHUv//v3p1atXoX116NCBUaNG0alTJzp37syYMWNo3979MQ4nTZpEmzZtuPrqq+natSsxMTGMGTOG6OhoOnTowNVXX80DDzxAbm4ubdu2JTg4mJiYmGInMks7BldddRVPPfUUPXr0ICYmhsceewyA119/ndWrV9OmTRs6duzIjh07CA0N5emnn6ZTp0706dOnzOM4depU7rjjDjp27FjovMWUKVM4efIkV199NTExMaxevdq5bOjQoXTr1s3ZnXOhpPDQNPaLjY015V2n6xHfvAxfT4fez8C1j3n/+VSVsXPnTlq31oaDOn8DBw5k4sSJ9O7du8TlJb3HRGSjY0j4Yqpmi/6/S+Drv8HVQ6D7xPLXV0opHzh16hQtW7akevXqpYb8+ah6Zx6P7rDu+XpxDAx+U28gopSqNOrVq8fu3bs9vt+q1aJPPwEJw6xhDYbNhdDq5W+jlFJ+ruq06PNyYN4I625Ro76EuiXe9lYppQJO1Qn6ZX+G/d/ALW/DpX+0uxqllPKZqtF1kzQLNrwLXR+Gdp65LlUppfxF4Af9/u9gyST4ww1wwzS7q1HK61JSUmjXrh3t2rWjUaNGNG7c2DmdnZ3t1j5Gjx7Nrl27ylznrbfeYs6cOZ4ouUJWrVrF+vXrff68/iywu25O/gbz7oGI5nD7exAUbHdFSnld/fr1+emnnwDrwzq1atXiiSeeKLSOMQZjDEGl3FTn/fffL/d5HnrooQsv9jysWrWKqKgounTpYsvzF8jLyyM42D8yJXCDPivNGt4gLxfiEqG6b8fDVgqApZPhf9s8u89GbWDACxXebO/evQwaNIj27duzefNmVqxYwbRp09i0aRMZGRnceeedPP20dSvo7t278+abb3L11VcTFRXFuHHjWLp0KTVq1GDRokU0bNiQKVOmEBUVxaOPPkr37t3p3r07q1atIjU1lffff5+uXbty9uxZRowYwc6dO4mOjmb//v3MnDnTOYBZgUmTJvHll18SEhLCgAED+Mc//sHRo0cZP348Bw4cICgoiDfeeIMGDRowc+ZMgoOD+eCDD/j3v/9N165dnftZv349EydOJDMzkxo1avDBBx/QokULcnNzmTRpEitWrCAoKIhx48bx4IMP8sMPP/Doo4+Snp5OeHg4q1evZu7cufz888/OoYL79+/PlClT6NKlC1FRUYwaNYpVq1bxzjvvsGzZMpYsWUJGRgbdu3cnPj4eEWH37t2MGzeOlJQUgoOD+eSTT/jLX/7C8OHDGThwIGANfDZixAhuuumm830nuC0wgz4/Hz59AH7fAXfNh6g/2F2RUpXCf//7X2bPnk1srPUByhdeeIHIyEhyc3Pp1asXQ4YMITo6utA2qamp9OjRgxdeeIHHHnuMWbNmMXny5GL7Nsbw448/snjxYqZPn86yZcv417/+RaNGjVi4cCFbtmwpNMxxgaNHj7JkyRK2b9+OiDiHIH7kkUf405/+RJcuXQqNkT9mzBjnH5iiWrduzTfffENISAjLli1jypQpfPzxx8THx3P48GG2bNlCcHAwJ06cIDMzk2HDhrFw4UI6dOhAampqocHVSpKamsp1113n/CNw5ZVXMm3aNIwxDB8+nGXLljFgwADi4uKYOnUqN998M5mZmeTn53PfffcRHx/PwIEDOXnyJBs2bPDZzVECM+j/8w/47xfWkMN/uMHualRVdh4tb2+64oornCEPkJCQwHvvvUdubi6HDx9mx44dxYK+evXqDBgwALCGEP7mm29K3HfB0MGuwwx/++23PPnkk4A1Ps5VVxUftjkyMpKgoCDuv/9+brrpJmeLd+XKlYXOE5w8ebLEUS1dnTp1ihEjRvDLL78Umr9y5UoeffRRZ1dLZGQkmzdvpmnTps4/PnXr1i1z3wBhYWHceuutzumvv/6aF198kczMTI4fP07Hjh3p0qULx48f5+abbwas8e/BGjRtwoQJpKSkkJCQwNChQ33W9RN4Qb/9M/jPC9DuLujyoN3VKFWpuA6fu2fPHl5//XV+/PFH6tWrx913313icL2uY7sHBweXenu8gtZwWeuUJDQ0lKSkJFasWMH8+fOJj4/nq6++cv6HUNbY8kU99dRT9OvXjwcffJC9e/fSv39/t7ctUNYQxNWrV3fepCU9PZ0JEyawadMmGjduzJQpU8oc7lhEuPvuu5k7dy4ffvihT09kB9ZVN0e2wmfjoUknGPiqDm+gVBlOnz5N7dq1qVOnDkeOHCl2Q3FP6NatG/PmzQNg27Zt7Nixo9g6Z86c4fTp0wwcOJBXX32VzZs3A3DDDTcUGsa34ARzeUMQFwwJ/MEHHzjn9+nTh7fffpu8vDzAGoI4OjqaAwcOsGnTJsA6Hnl5eTRr1ozNmzdjjGH//v1s3LixxOfKyMggKCiIqKgozpw5w8KFCwHrBiINGjTg888/B6w/FAX30B09ejQvvvgi1apV48orr3TjCHpG4AR92jHr5Gv1COvG3iFl97UpVdV16NCB6OhoWrVqxYgRIwoNNewpDz/8MIcOHSI6Oppp06YRHR1drIskNTWVm266iZiYGHr06MErr7wCWJdvfvfdd7Rt25bo6GjeffddwLqpyrx582jfvj3r1q0rtK8nn3ySSZMm0aFDB1xH5n3ggQdo1KgRbdu2JSYmhnnz5lGtWjUSEhIYP348MTEx9O3bl6ysLHr06EHjxo1p3bo1jz/+eLETxwXq16/PyJEjiY6OZsCAAXTu3Nm5bM6cObz88su0bduW7t27c+zYMcC68XrLli09dkMRdwXOMMXpJ+CzB6Hnk3CJ+2NcK+VpOkzxObm5ueTm5hIeHs6ePXvo27cve/bsISQk8HqN3XH27FnatGnDli1bqF279nnvp6LDFAfO0a4RCcMTy19PKeUzaWlp9O7dm9zcXIwxvPPOO1U25JcvX87999/PpEmTLijkz0fVPOJKKZ+oV69eqX3cVU2/fv04cOCALc/tVh+9iPQXkV0isldEil1AKyKvishPjq/dInLKZdlIEdnj+BrpyeKVqqwqW5eoChzn894qt0UvIsHAW0AfIBnYICKLjTHO0+fGmIku6z8MtHc8jgSeAWIBA2x0bHuywpUq5SfCw8NJSUmhfv36zkvxlPIEYwwpKSnOa/Pd5U7XTSdgrzFmH4CIJAKDgeLXSVnisMIdoB+wwhhzwrHtCqA/kFChKpXyI02aNCE5Odl5pYVSnhQeHk6TJk0qtI07Qd8YOOgynQx0LmlFEbkMaA6sKmPbYnf8EJGxwFiApk2bulGSUpVXaGgozZs3t7sMpZw8fR39MGCBMSavIhsZY2YYY2KNMbENGjTwcElKKVW1uRP0h4BLXaabOOaVZBiFu2Uqsq1SSikvcCfoNwAtRKS5iIRhhfnioiuJSCsgAvjeZfZyoK+IRIhIBNDXMU8ppZSPlNtHb4zJFZEJWAEdDMwyxmwXkelAkjGmIPSHAYnG5dofY8wJEfkb1h8LgOkFJ2ZLs3HjxuMi8tv5vBiHKOD4BWzvLVpXxWhdFaN1VUwg1nVZaQsq3RAIF0pEkkr7GLCdtK6K0boqRuuqmKpWV+AMaqaUUqpEGvRKKRXgAjHoZ9hdQCm0rorRuipG66qYKlVXwPXRK6WUKiwQW/RKKaVcaNArpVSA85ugd2Oo5Goi8rFj+Q8i0sxl2Z8d83eJSD8f1/WYiOwQka0i8rVjPKCCZXkuwzsX+xCal+saJSLHXJ5/jMsyrw0tfYFDXnvzeM0Skd9F5OdSlouIvOGoe6uIdHBZ5s3jVV5ddznq2SYi60QkxmXZfsf8n0TkPG7bdkF19RSRVJef19Muy8p8D3i5rkkuNf3seE9FOpZ583hdKiKrHVmwXUT+XwnreO89Zoyp9F9YH9T6BbgcCAO2ANFF1nkQeNvxeBjwseNxtGP9algDrv0CBPuwrl5ADcfj8QV1OabTbDxeo4A3S9g2Etjn+B7heBzhq7qKrP8w1gf0vHq8HPu+DugA/FzK8huBpYAAXYAfvH283Kyra8HzAQMK6nJM7weibDpePYEvLvQ94Om6iqx7M7DKR8frYqCD43FtYHcJv5Nee4/5S4veOVSyMSYbKBgq2dVg4EPH4wVAbxERx/xEY0yWMeZXYK9jfz6pyxiz2hiT7phcjzXej7e5c7xK4xxa2lj3DSgYWtqOuuLw0ZDWxpi1QFmf2h4MzDaW9UA9EbkY7x6vcusyxqwz5+7v4Kv3lzvHqzQX8t70dF2+fH8dMcZscjw+A+yk+Ei+XnuP+UvQuzPcsXMdY0wukArUd3Nbb9bl6j6sv9gFwkUkSUTWi8gtHqqpInXd7vgXcYGIFAw+VymOlxQf8hq8d7zcUVrt3jxeFVX0/WWAr0Rko1hDgfvaNSKyRUSWishVjnmV4niJSA2ssFzoMtsnx0usbuX2wA9FFnntPab3jPUREbkb605bPVxmX2aMOSQilwOrRGSbMeYXH5X0OZBgjMkSkQew/hu63kfP7Y6Shry283hVaiLSCyvou7vM7u44Xg2BFSLyX0eL1xc2Yf280kTkRuAzoIWPntsdNwPfmcJjb3n9eIlILaw/Lo8aY057ct9l8ZcWvTvDHTvXEZEQoC6Q4ua23qwLEbkBeAoYZIzJKphvjDnk+L4PWIPjFoy+qMsYk+JSy0ygo7vberMuF0WHvPbm8XJHabXbPhS3iLTF+hkONsakFMx3OV6/A5/iuS7LchljThtj0hyPlwChIhJFJTheDmW9v7xyvEQkFCvk5xhjPilhFe+9x7xx4sHTX1j/eezD+le+4ATOVUXWeYjCJ2PnOR5fReGTsfvw3MlYd+pqj3XyqUWR+RFANcfjKGAPHjop5WZdF7s8vhVYb86d+PnVUV+E43Gkr+pyrNcK68SY+OJ4uTxHM0o/uXgThU+U/ejt4+VmXU2xzjt1LTK/JlDb5fE6oL8P62pU8PPDCswDjmPn1nvAW3U5ltfF6sev6avj5Xjts4HXyljHa+8xjx1cb39hnZHejRWaTznmTcdqJQOEA/Mdb/ofgctdtn3Ksd0uYICP61oJHAV+cnwtdszvCmxzvNG3Aff5uK7nge2O518NtHLZ9l7HcdwLjPZlXY7pqcALRbbz9vFKAI4AOVh9oPcB44BxjuUCvOWoexsQ66PjVV5dM4GTLu+vJMf8yx3Haovj5/yUj+ua4PL+Wo/LH6KS3gO+qsuxziisCzRct/P28eqOdQ5gq8vP6kZfvcd0CASllApw/tJHr5RS6jxp0CulVIDToFdKqQCnQa+UUgFOg14ppQKcBr1SSgU4DXqllApw/x+4l4P9AjwXmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    train_metric = 'acc' if 'acc' in history.history else 'accuracy'\n",
    "    val_metric = 'val_acc' if 'val_acc' in history.history else 'val_accuracy'\n",
    "    plt.plot(history.history[val_metric],label=\"Validation set accuracy\")\n",
    "    plt.plot(history.history[train_metric],label=\"Training set accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not a bad result. However, there are caveats:\n",
    "\n",
    "* Both the training and development datasets were downsampled and truncated, so the results are not comparable to ones from the full dataset\n",
    "* The metric makes no distinction between \"real\" tags and placeholders added as padding and for subwords. The result is expected to overestimate performance in particular for padded inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with the trained model\n",
    "\n",
    "Let's run a few simple cases through the trained model to illustrate its use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\tDET\n",
      "model\tNOUN\n",
      "can\tAUX\n",
      "predict\tVERB\n",
      "parts\tNOUN\n",
      "of\tADP\n",
      "speech\tNOUN\n",
      ".\tPUNCT\n",
      "\n",
      "What\tPRON\n",
      "would\tAUX\n",
      "the\tDET\n",
      "best\tADJ\n",
      "time\tNOUN\n",
      "be\tAUX\n",
      "tomorrow,\tNOUN\n",
      "John\tPROPN\n",
      "wondered\tVERB\n",
      "quietly\tADV\n",
      ".\tPUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_tags(words):\n",
    "    # This function takes a sequence of words, vectorizes it,\n",
    "    # and returns the model predictions. The vectorization code\n",
    "    # we wrote expects labels, so we'll attach dummy labels to\n",
    "    # use the code and then discard the dummy labels it generates.\n",
    "    dummy_tagged = [(word, NO_LABEL) for word in words]\n",
    "    tokenized_test = tokenize_sentences([dummy_tagged])\n",
    "    test_X, dummy_Y = vectorize_dataset(tokenized_test)\n",
    "    # Run model.predict for this single sequence \n",
    "    predictions = model.predict(test_X)\n",
    "    # Our outputs are one-hot, take argmax to get indices\n",
    "    y = predictions[0].argmax(axis=1)\n",
    "    # For words tokenized into several subword parts, we\n",
    "    # only care about the predicted tag for the first part.\n",
    "    # Use the tokenization to identify these parts.\n",
    "    tags = []\n",
    "    i = 1    # Start at 1 to skip the special [CLS] token.\n",
    "    for tokens, _ in tokenized_test[0]:\n",
    "        tags.append(int_to_tag[y[i]])\n",
    "        i += len(tokens)\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Test the model with a few word sequences\n",
    "test_words = [\n",
    "  'This model can predict parts of speech .'.split(),\n",
    "  'What would the best time be tomorrow, John wondered quietly .'.split()\n",
    "]\n",
    "\n",
    "\n",
    "for words in test_words:\n",
    "    tags = predict_tags(words)\n",
    "    for w, t in zip(words, tags):\n",
    "        print('{}\\t{}'.format(w, t))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simple model, written in functional API and without binary crossentropy\n",
    "\n",
    "Task 1.\n",
    "Use this to play with the different parameters.\n",
    "How much do they affect the end result?\n",
    "What did you try?\n",
    "\n",
    "Task 2. More layers usually means more power. Can you add another convolution layer to the model? Did it work for you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print (x_train.shape)\n",
    "\n",
    "#Since we are using output the size of 2, we will have to do one-hot encoding\n",
    "#x_test = to_categorical(x_test)\n",
    "#y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_test = onehot_encoder.fit_transform(y_test.reshape(-1, 1))\n",
    "y_train = onehot_encoder.transform(y_train.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "#Let's define the inputs\n",
    "x = Input(shape=(maxlen,))\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "\n",
    "embedding_layer = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen)\n",
    "\n",
    "embeddings = embedding_layer(x)\n",
    "\n",
    "conv_layer = Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)\n",
    "conv_result = conv_layer(embeddings)\n",
    "pooled = (GlobalMaxPooling1D())(conv_result) \n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "out = Dense(2, activation='softmax')(pooled)\n",
    "\n",
    "model = Model(x, out)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
